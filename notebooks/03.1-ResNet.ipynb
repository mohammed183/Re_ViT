{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the ResNet model\n",
    "\n",
    "In this notebook we use the pretrained **ResNet-152x4** model on the **ImageNet-21k** dataset which contains about 14 million images. The model will be finetuned on different datasets which are used for image classification tasks and then used as baseline model:\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "0fe32403-52ed-4e00-9cb1-cec65d518e5c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the required modules:"
   ],
   "id": "059944a1-6b7b-42d2-a7ac-5fa963462407"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from torchvision import transforms, datasets, models"
   ],
   "id": "94c6dde7-edb2-49c2-8878-ab46e22564d0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We need to create the dataloaders that will help us train our **ResNet** model. The function `get_res_loaders` can load five different datasets for us. We can select the dataset we want by passing its name to the `dataset` argument. We can also adjust the `batch_size` argument according to the GPU we have."
   ],
   "id": "1669e1fc-6757-41ef-88b4-a3f4dc5735e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_dataset_sizes = {\n",
    "  'cifar10': (128, 128),\n",
    "  'cifar100': (128, 128),\n",
    "  'oxford_pets': (128, 128),\n",
    "  'flowers_102': (128, 128),\n",
    "  'imagenet': (384, 384),\n",
    "}\n",
    "\n",
    "def get_res_loaders(dataset=\"imagenet\", batch_size=64):\n",
    "    \"\"\"\n",
    "    This loads the whole dataset into memory and returns train and test data to\n",
    "    be used by the ResNet model\n",
    "    @param dataset (string): dataset name to load\n",
    "    @param batch_size (int): batch size for training and testing\n",
    "\n",
    "    @returns dict() with train and test data loaders with keys `train_loader`, `test_loader`\n",
    "    \"\"\"\n",
    "    # Get image size\n",
    "    crop = known_dataset_sizes[dataset]\n",
    "    precrop = (160,160)\n",
    "    # Normalization using channel means\n",
    "    normalize_transform = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "    # Creating transform function\n",
    "    train_transform = transforms.Compose([\n",
    "                        transforms.Resize(precrop),\n",
    "                        transforms.RandomCrop(crop),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ToTensor(),\n",
    "                        normalize_transform,\n",
    "                    ])\n",
    "\n",
    "    # Test transformation function\n",
    "    test_transform =transforms.Compose([transforms.Resize(crop), transforms.ToTensor(), normalize_transform])\n",
    "\n",
    "    # Load the dataset from torchvision datasets\n",
    "    if dataset == \"imagenet\":\n",
    "        # Load ImageNet\n",
    "        original_train_dataset = None\n",
    "        train_loader = None\n",
    "\n",
    "        original_test_dataset = datasets.ImageFolder(root=os.path.join('data', 'imagenet', 'val'), transform=test_transform)\n",
    "\n",
    "    elif dataset == \"cifar10\":\n",
    "        # Load CIFAR-10\n",
    "        original_train_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == \"cifar100\":\n",
    "        # Load CIFAR-100\n",
    "        original_train_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == \"oxford_pets\":\n",
    "        # Load Oxford-IIIT Pets\n",
    "        original_train_dataset = datasets.OxfordIIITPet(root=os.path.join('data', 'oxford_iiit_pets_data'),\n",
    "                                             split='trainval', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.OxfordIIITPet(root=os.path.join('data', 'oxford_iiit_pets_data'),\n",
    "                                             split='test', transform=test_transform, download=True)\n",
    "    elif dataset == \"flowers_102\":\n",
    "        # Load Oxford Flowers-102\n",
    "        original_train_dataset = datasets.Flowers102(root=os.path.join('data', 'oxford_flowers_102_data'),\n",
    "                                             split='train', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.Flowers102(root=os.path.join('data', 'oxford_flowers_102_data'),\n",
    "                                             split='test', transform=test_transform, download=True)\n",
    "    else:\n",
    "        # Raise an error if the dataset is not valid\n",
    "        raise ValueError(\"Invalid dataset name. Please choose one of the following: imagenet, cifar10, cifar100, oxford_pets, flowers_102\")\n",
    "\n",
    "    # Creating data loaders\n",
    "    loader_args = {\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "    if original_train_dataset is not None:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=original_train_dataset,\n",
    "            shuffle=True,\n",
    "            **loader_args)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_test_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    return {\"train_loader\": train_loader,\n",
    "            \"test_loader\": test_loader}"
   ],
   "id": "33f5e636-892f-4f10-bc69-c57003aa47f9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Then we use the **ResNet** implementation from the [official repository](https://github.com/google-research/big_transfer) to create our model."
   ],
   "id": "0adfe5c2-3aca-4c67-814a-98e7a27a8d7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StdConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    A custom convolutional layer that standardizes the weights before applying them.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        w = self.weight\n",
    "        # Compute the variance and mean of the weights along the channel, height and width dimensions\n",
    "        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
    "        # Standardize the weights by subtracting the mean and dividing by the standard deviation\n",
    "        w = (w - m) / torch.sqrt(v + 1e-10)\n",
    "        # Apply the standardized weights\n",
    "        return F.conv2d(x, w, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "# helper function to create a 3x3 convolutional layer with standardization.    \n",
    "def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=3, stride=stride, padding=1, bias=bias, groups=groups)\n",
    "\n",
    "# helper function to create a 1x1 convolutional layer with standardization.\n",
    "def conv1x1(cin, cout, stride=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=1, stride=stride, padding=0, bias=bias)\n",
    "\n",
    "# helper function to convert the weight format from TensorFlow (HWIO) to PyTorch (OIHW).\n",
    "def tf2th(conv_weights):\n",
    "    if conv_weights.ndim == 4:\n",
    "        # Transpose the dimensions from height-width-input-output to output-input-height-width\n",
    "        conv_weights = np.transpose(conv_weights, [3, 2, 0, 1])\n",
    "    return torch.from_numpy(conv_weights)\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom residual block that uses pre-activation and group normalization.\n",
    "    This is based on the paper \"Identity Mappings in Deep Residual Networks\" by Kaiming He et al.\n",
    "    https://arxiv.org/abs/1603.05027\n",
    "    However, this implementation differs from the original one by putting\n",
    "    the stride on the 3x3 convolution instead of the 1x1 convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, cin, cout=None, cmid=None, stride=1):\n",
    "        super().__init__()\n",
    "        # Set the output channels to be the same as the input channels if not specified\n",
    "        cout = cout or cin\n",
    "        # Set the middle channels to be one fourth of the output channels if not specified\n",
    "        cmid = cmid or cout//4\n",
    "\n",
    "        # Define the group normalization and convolution layers for the block\n",
    "        self.gn1 = nn.GroupNorm(32, cin)\n",
    "        self.conv1 = conv1x1(cin, cmid)\n",
    "        self.gn2 = nn.GroupNorm(32, cmid)\n",
    "        self.conv2 = conv3x3(cmid, cmid, stride)  # Original ResNetv2 has it on conv1!!\n",
    "        self.gn3 = nn.GroupNorm(32, cmid)\n",
    "        self.conv3 = conv1x1(cmid, cout)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Define the projection layer for the residual branch if needed\n",
    "        if (stride != 1 or cin != cout):\n",
    "            self.downsample = conv1x1(cin, cout, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv'ed branch\n",
    "        out = self.relu(self.gn1(x))\n",
    "\n",
    "        # Residual branch\n",
    "        residual = x\n",
    "        # If there is a projection layer, apply it to the output of the first activation\n",
    "        if hasattr(self, 'downsample'):\n",
    "            residual = self.downsample(out)\n",
    "\n",
    "        # The first block has already applied pre-act before splitting, see Appendix.\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(self.relu(self.gn2(out)))\n",
    "        out = self.conv3(self.relu(self.gn3(out)))\n",
    "        \n",
    "        # Add the residual branch to the conv'ed branch and return\n",
    "        return out + residual\n",
    "    \n",
    "    # helper function to load the weights from a TensorFlow model\n",
    "    def load_from(self, weights, prefix=''):\n",
    "        with torch.no_grad():\n",
    "            # Copy the weights for each layer from the TensorFlow model to the PyTorch model\n",
    "            self.conv1.weight.copy_(tf2th(weights[prefix + 'a/standardized_conv2d/kernel']))\n",
    "            self.conv2.weight.copy_(tf2th(weights[prefix + 'b/standardized_conv2d/kernel']))\n",
    "            self.conv3.weight.copy_(tf2th(weights[prefix + 'c/standardized_conv2d/kernel']))\n",
    "            self.gn1.weight.copy_(tf2th(weights[prefix + 'a/group_norm/gamma']))\n",
    "            self.gn2.weight.copy_(tf2th(weights[prefix + 'b/group_norm/gamma']))\n",
    "            self.gn3.weight.copy_(tf2th(weights[prefix + 'c/group_norm/gamma']))\n",
    "            self.gn1.bias.copy_(tf2th(weights[prefix + 'a/group_norm/beta']))\n",
    "            self.gn2.bias.copy_(tf2th(weights[prefix + 'b/group_norm/beta']))\n",
    "            self.gn3.bias.copy_(tf2th(weights[prefix + 'c/group_norm/beta']))\n",
    "            # If there is a projection layer, copy its weights as well\n",
    "            if hasattr(self, 'downsample'):\n",
    "                self.downsample.weight.copy_(tf2th(weights[prefix + 'a/proj/standardized_conv2d/kernel']))\n",
    "        \n",
    "        # Return the PyTorch model with loaded weights\n",
    "        return self\n",
    "\n",
    "# ResNet Class\n",
    "class ResNetV2(nn.Module):\n",
    "    BLOCK_UNITS = {\n",
    "        'r50': [3, 4, 6, 3],\n",
    "        'r101': [3, 4, 23, 3],\n",
    "        'r152': [3, 8, 36, 3],\n",
    "    }\n",
    "\n",
    "    def __init__(self, block_units, width_factor, head_size=21843, zero_head=False):\n",
    "        super().__init__()\n",
    "        wf = width_factor  # shortcut 'cause we'll use it a lot.\n",
    "\n",
    "        self.root = nn.Sequential(OrderedDict([\n",
    "            ('conv', StdConv2d(3, 64*wf, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            ('padp', nn.ConstantPad2d(1, 0)),\n",
    "            ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0)),\n",
    "            # The following is subtly not the same!\n",
    "            #('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        self.body = nn.Sequential(OrderedDict([\n",
    "            ('block1', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin= 64*wf, cout=256*wf, cmid=64*wf))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=256*wf, cout=256*wf, cmid=64*wf)) for i in range(2, block_units[0] + 1)],\n",
    "            ))),\n",
    "            ('block2', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin=256*wf, cout=512*wf, cmid=128*wf, stride=2))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=512*wf, cout=512*wf, cmid=128*wf)) for i in range(2, block_units[1] + 1)],\n",
    "            ))),\n",
    "            ('block3', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin= 512*wf, cout=1024*wf, cmid=256*wf, stride=2))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=1024*wf, cout=1024*wf, cmid=256*wf)) for i in range(2, block_units[2] + 1)],\n",
    "            ))),\n",
    "            ('block4', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin=1024*wf, cout=2048*wf, cmid=512*wf, stride=2))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=2048*wf, cout=2048*wf, cmid=512*wf)) for i in range(2, block_units[3] + 1)],\n",
    "            ))),\n",
    "        ]))\n",
    "\n",
    "        self.zero_head = zero_head\n",
    "        self.head = nn.Sequential(OrderedDict([\n",
    "            ('gn', nn.GroupNorm(32, 2048*wf)),\n",
    "            ('relu', nn.ReLU(inplace=True)),\n",
    "            ('avg', nn.AdaptiveAvgPool2d(output_size=1)),\n",
    "            ('conv', nn.Conv2d(2048*wf, head_size, kernel_size=1, bias=True)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.head(self.body(self.root(x)))\n",
    "        assert x.shape[-2:] == (1, 1)  # We should have no spatial shape left.\n",
    "        return x[...,0,0]\n",
    "\n",
    "    def load_from(self, weights, prefix='resnet/'):\n",
    "        with torch.no_grad():\n",
    "            self.root.conv.weight.copy_(tf2th(weights[f'{prefix}root_block/standardized_conv2d/kernel']))\n",
    "            self.head.gn.weight.copy_(tf2th(weights[f'{prefix}group_norm/gamma']))\n",
    "            self.head.gn.bias.copy_(tf2th(weights[f'{prefix}group_norm/beta']))\n",
    "            if self.zero_head:\n",
    "                nn.init.zeros_(self.head.conv.weight)\n",
    "                nn.init.zeros_(self.head.conv.bias)\n",
    "            else:\n",
    "                self.head.conv.weight.copy_(tf2th(weights[f'{prefix}head/conv2d/kernel']))\n",
    "                self.head.conv.bias.copy_(tf2th(weights[f'{prefix}head/conv2d/bias']))\n",
    "\n",
    "            for bname, block in self.body.named_children():\n",
    "                for uname, unit in block.named_children():\n",
    "                    unit.load_from(weights, prefix=f'{prefix}{bname}/{uname}/')\n",
    "        return self"
   ],
   "id": "7be5d5e2-2ae1-438c-8234-7cffa7dd4f99"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following are two helper functions that we use for calculating and evaluating the model performance:\n",
    "\n",
    "-   `get_accuracy`: This function takes the model predictions and the true labels as inputs and returns the accuracy as a float value.\n",
    "-   `evaluate_on_test`: This function takes the model, the test dataloader, and the device as inputs and returns the test accuracy and loss as float values."
   ],
   "id": "24237255-2a43-4144-8967-a0c6d1ecd3e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes predictions and true values to return accuracies\n",
    "def get_accuracy(logit, true_y):\n",
    "    pred_y = torch.argmax(logit, dim=1)\n",
    "    return (pred_y == true_y).float().mean()\n",
    "\n",
    "# This Function is used to evaluate the model\n",
    "def evaluate_on_test(model, test_loader, device):\n",
    "    # Evaluate the model on all the test batches\n",
    "    accuracies = []\n",
    "    model.eval()\n",
    "    for batch_idx, (data_x, data_y) in enumerate(test_loader):\n",
    "        data_x = data_x.to(device)\n",
    "        data_y = data_y.to(device)\n",
    "\n",
    "        model_y = model(data_x)\n",
    "        batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "        accuracies.append(batch_accuracy.item())\n",
    "\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print(f\"Mean accuracy at batch: {batch_idx} is {np.mean(accuracies) * 100}\")\n",
    "\n",
    "    test_accuracy = np.mean(accuracies) * 100\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "    return test_accuracy"
   ],
   "id": "7d46f78b-107e-4875-85ab-f45a5c87895f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The function `train_res_model` takes the following arguments and returns the train and test accuracies of the model:\n",
    "\n",
    "-   `loaders`: A dictionary of PyTorch dataloaders for the train and test sets.\n",
    "-   title: A string to label the plot of the training and validation losses.\n",
    "-   `model_name`: A string to specify the name of the ResNet model to use. The default is ‘r152’, which corresponds to ResNet-152.\n",
    "-   `bit_model`: A string to specify the name of the pretrained Big Transfer (BiT) model to use. The default is “BiT-M-R152x4”, which corresponds to BiT-M with ResNet-152 backbone and width factor 4.\n",
    "-   `width_factor`: An integer to scale the width of the ResNet model. The default is 4, which means the number of channels in each layer is multiplied by 4.\n",
    "-   `lr`: A float to set the learning rate for the optimizer. The default is 0.001.\n",
    "-   `epochs`: An integer to set the number of epochs for the training loop. The default is 10.\n",
    "\n",
    "The function first creates a ResNet model with the specified arguments and loads the weights from the pretrained BiT model. Then, it creates an optimizer and a scheduler for the training process. Finally, it runs a training loop for the given number of epochs, where it computes the loss and accuracy for each batch of data, updates the model parameters, and adjusts the learning rate."
   ],
   "id": "581175e5-2268-4431-9a62-600d800b1105"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model and return train and test accuracies\n",
    "def train_res_model(loaders, title='', model_name='r152', bit_model=\"BiT-M-R152x4\",\n",
    "                       width_factor=4, lr=0.0003, epochs=10, random_seed=42, save=False):\n",
    "\n",
    "    # Create experiment directory name if none\n",
    "    experiment_dir = os.path.join('experiments', title)\n",
    "\n",
    "    # make experiment directory\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "    # Set the seed\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Get num_classes\n",
    "    num_classes = len(loaders[\"train_loader\"].dataset.classes)\n",
    "\n",
    "    # Get weights\n",
    "    print(f\"Loading {bit_model} weights...\")\n",
    "    weights = get_weights(bit_model)\n",
    "    print(\"Weight successfully loaded\")\n",
    "\n",
    "    # Initialize the ResNet model\n",
    "    model = ResNetV2(ResNetV2.BLOCK_UNITS[model_name], width_factor=width_factor, head_size=num_classes, zero_head=True)\n",
    "    model.load_from(weights)\n",
    "    model.to(device);\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Calculate batch size\n",
    "    batch_size = loaders[\"train_loader\"].batch_size\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Arrays to hold accuracies\n",
    "    test_acc = []\n",
    "    train_acc = []\n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Make model params trainable\n",
    "        model.to(device);\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "\n",
    "        # Calculate loss and gradients for models on every training batch\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"train_loader\"]):\n",
    "            data_x = data_x.to(device)\n",
    "            data_y = data_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model_y = model(data_x)\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "            # Perform back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Store training accuracy for plotting\n",
    "        train_loss = np.mean(losses)\n",
    "        train_accuracy = np.mean(accuracies)*100\n",
    "        train_acc.append(train_accuracy)\n",
    "\n",
    "        print(\"Train accuracy: {} Train loss: {}\".format(train_accuracy, train_loss))\n",
    "\n",
    "        # Evaluate the model on all the test batches\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        # Move the model to CPU\n",
    "        model.to(\"cpu\")\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"test_loader\"]):\n",
    "            # Move the data to CPU\n",
    "            data_x = data_x.to(\"cpu\")\n",
    "            data_y = data_y.to(\"cpu\")\n",
    "\n",
    "            model_y = model(data_x)\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Break if there are more than 500 samples and not last iteration\n",
    "            if (batch_idx+1)*batch_size > 200 and epoch < epochs:\n",
    "                break\n",
    "\n",
    "        # Store test accuracy for plotting\n",
    "        test_loss = np.mean(losses)\n",
    "        test_accuracy = np.mean(accuracies)\n",
    "        test_acc.append(test_accuracy*100)\n",
    "        print(\"Test accuracy: {} Test loss: {}\".format(test_accuracy*100, test_loss))\n",
    "\n",
    "\n",
    "    if save:\n",
    "        # Save the final model\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, os.path.join(experiment_dir, f'{bit_model}.pt'))\n",
    "\n",
    "    # Delete the data and model outputs from GPU memory\n",
    "    del model, optimizer\n",
    "    # Release unused memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # return the accuracies\n",
    "    return train_acc, test_acc"
   ],
   "id": "9e212e87-07fc-404f-9a08-1811d657727b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following function `plot_images_from_dataloader` takes a PyTorch dataloader as an input and plots 10 of the images from the first batch of data. The function also shows the labels of the images according to the classes attribute of the dataloader’s dataset"
   ],
   "id": "283c6810-5e75-4ed3-bf3a-2ac94e169931"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot 10 of the images\n",
    "def plot_images_from_dataloader(dataloader):\n",
    "    # Initialize empty tensors for images and labels\n",
    "    images = torch.empty(0)\n",
    "    labels = torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    # Loop until the images and labels have at least 10 elements\n",
    "    while len(images) < 10:\n",
    "        # Get the next batch of images and labels from the dataloader\n",
    "        batch_images, batch_labels = next(iter(dataloader))\n",
    "        # Concatenate the batch images and labels to the existing tensors\n",
    "        images = torch.cat((images, batch_images), dim=0)\n",
    "        labels = torch.cat((labels, batch_labels), dim=0)\n",
    "    classes = dataloader.dataset.classes\n",
    "    # Create a figure with 2 rows and 5 columns\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = images[i]\n",
    "        label = classes[labels[i]]\n",
    "        # Unnormalize the image\n",
    "        image = image / 2 + 0.5\n",
    "        image = image.numpy()\n",
    "        # Transpose the image\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        # Plot the image on the axis\n",
    "        ax.imshow(image)\n",
    "        # Set title as label\n",
    "        ax.set_title(label)\n",
    "        # Turn off the axis ticks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()"
   ],
   "id": "9e87445e-c5b2-4500-8283-7914d5e3ea47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to load npz files\n",
    "def get_weights(bit_variant):\n",
    "    return np.load(f'pretrained_models/{bit_variant}.npz')\n",
    "\n",
    "# Function used to calculated time\n",
    "def print_time(start_time, end_time):\n",
    "    # Calculate the difference in seconds\n",
    "    diff = end_time - start_time\n",
    "\n",
    "    # Convert the difference to hours, minutes, and seconds\n",
    "    hours, remainder = divmod(diff, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "    # Print the time in hours:minutes:seconds format\n",
    "    print(f\"Cell execution time: {int(hours)}:{int(minutes)}:{seconds}\")"
   ],
   "id": "69902c05-94e1-41dd-94e6-b505349d56eb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We download the models that we will be using through the rest of the notebook:\n",
    "\n",
    "-   The `BiT-M-R152x4` is pretrained on the **ImageNet-21k** and not fine tuned at all.\n",
    "-   The `BiT-M-R152x4-ILSVRC2012` is pretreained on the **ImageNet-21k** and fine tuned on the **ImageNet-1k**"
   ],
   "id": "4dce8824-ced2-4d1a-a9cf-299abfbf5238"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to hold the pretrained models\n",
    "!mkdir -p pretrained_models\n",
    "\n",
    "# Download the ResNet152x4 model\n",
    "![ -e pretrained_models/BiT-M-R152x4.npz ] || \\\n",
    "curl -L -o pretrained_models/BiT-M-R152x4.npz \"https://storage.googleapis.com/bit_models/BiT-M-R152x4.npz\"\n",
    "\n",
    "# Download the ResNet152x4 model fine tuned on ImageNet\n",
    "![ -e pretrained_models/BiT-M-R152x4-ILSVRC2012.npz ] || \\\n",
    "curl -L -o pretrained_models/BiT-M-R152x4-ILSVRC2012.npz \"https://storage.googleapis.com/bit_models/BiT-M-R152x4-ILSVRC2012.npz\""
   ],
   "id": "d65b9002-fc13-4f68-9ff5-944458d8427b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "e205df09-1e4d-4fe9-96ca-38fb450f1bb7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch size chosen in the following runs are for a `RTX6000 GPU`. If you are using another GPU you can try changing it to maximize the usage of your GPU.\n",
    "\n",
    "You can use any of these commands in the terminal while running the cells to check how much of your GPU memory is utilized:\n",
    "\n",
    "-   `nvidia-smi --query-gpu=gpu_name,memory.used,memory.free,memory.total --format=csv`\n",
    "-   `nvidia-smi pmon -s m -c 1`\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "086ac08f-cd77-4680-a5bb-ad7e01dc1a65"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet\n",
    "\n",
    "The ImageNet dataset consists of **1000** object classes and contains **1,281,167** training images, **50,000** validation images and **100,000** test images. The images vary in resolution but it is common practice to train deep learning models on sub-sampled images of **256x256**pixels. This dataset is widely used for image classification and localization tasks and has been the benchmark for many state-of-the-art algorithms.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "0c943137-4ace-408f-942d-e9d0d1d641e9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the dataset and choose the `batch_size` that suits our GPU capacity. This will help us avoid errors during the training process. Next, we use the `plot_images_from_dataloader` function to display 10 of the images from the first batch of data. We can see the labels of the images on top of each plot. However, we need to make sure that the batch size is at least 10, otherwise the function will raise an error."
   ],
   "id": "633dcead-a86d-41af-835e-7909cb10c7e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the ImageNet dataset\n",
    "loader = get_res_loaders(dataset=\"imagenet\", batch_size=1)\n",
    "plot_images_from_dataloader(loader[\"test_loader\"])"
   ],
   "id": "76b8980f-ebcf-45b8-ab79-5e3c2eb76c67"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "For the **ImageNet** dataset we already have the `BiT-M-R152x4-ILSVRC2012` which is already fine tuned on the dataset. We will create the model and load its weights and evaluate it on the test set."
   ],
   "id": "e61ca7e3-f403-4cc8-ad62-48504f00ad51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"CUDA Recognized\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Get num_classes\n",
    "num_classes = len(loader[\"test_loader\"].dataset.classes)\n",
    "\n",
    "# Get weights\n",
    "print(f\"Loading BiT-M-R152x4-ILSVRC2012 weights...\")\n",
    "weights = get_weights(\"BiT-M-R152x4-ILSVRC2012\")\n",
    "print(\"Weight successfully loaded\")\n",
    "\n",
    "# Initialize the ResNet model\n",
    "model = ResNetV2(ResNetV2.BLOCK_UNITS['r152'], width_factor=4, head_size=num_classes)\n",
    "model.load_from(weights)\n",
    "model.to(device);"
   ],
   "id": "c57042ef-9417-4e31-82eb-11bc5eba56f2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We use the `evaluate_on_test` function to get the train and test accuracies for the model."
   ],
   "id": "a4c8046e-a079-454a-8539-7ec0025d679b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save start time\n",
    "start_time = time.time()\n",
    "# Print the Performance of the Ready fine tuned model\n",
    "test_acc_imagenet = evaluate_on_test(model, loader[\"test_loader\"], device)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "print_time(start_time, end_time)\n",
    "# delete model to free gpu\n",
    "del model\n",
    "# Release unused memory\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "628e5eff-edcc-434a-8976-6da05d2a4960"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save the results in a dictionary that will be used to create a table with the model’s results."
   ],
   "id": "596ec090-00fa-4bc0-9c2d-bbb9b3a9bf29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"imagenet\"] =  test_acc_imagenet"
   ],
   "id": "eb7883df-02bb-4a76-9fea-aaf2485d9ece"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/resnet.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ],
   "id": "ae128d86-58fc-4585-96cc-9b8019b22a8a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "593a504a-9c4f-4c33-b57e-4765644f977b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of **60,000 32x32** color images in **10** different classes. The 10 classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. There are **6,000** images per class, with **5,000** for training and **1,000** for testing. It is a popular benchmark for image classification and deep learning research.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "b51fe6ad-61d7-47bc-9014-7f04eae78140"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the **CIFAR-10** dataset and some of the images in it."
   ],
   "id": "b55fe782-c2a6-4d20-a71b-5ce2c91bcad2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the CIFAR-10 dataset\n",
    "loader = get_res_loaders(dataset=\"cifar10\", batch_size=16)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "e7cf6c5a-d535-442e-9a6a-ddcb401c03f3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We then fine-tune the model for 10 epochs on the dataset and get the train and test accuracies."
   ],
   "id": "9971d207-c2e8-46d0-81e5-a8d94d2efba5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on CIFAR-10\n",
    "train_acc_cifar10, test_acc_cifar10 = train_res_model(loaders=loader)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "print_time(start_time, end_time)"
   ],
   "id": "88b35778-9351-441b-ac5c-a588a8456dd7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save the results in the same dictionary as before."
   ],
   "id": "1a3a0859-8c6b-4b2e-b6c3-0fb358740787"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"cifar10\"] = test_acc_cifar10[-1]\n"
   ],
   "id": "1e317fc9-d1a7-4cf0-b6e4-14b97d58390c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/resnet.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ],
   "id": "8640b5c0-ed48-4427-b2e9-3c27c1bf95c3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "50ffefff-ad7e-44a9-83e9-713c4dc1adbb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-100\n",
    "\n",
    "The CIFAR-100 dataset consists of **60,000 32x32** color images in **100** different classes. The 100 classes are grouped into 20 superclasses, such as aquatic mammals, flowers, insects, vehicles, etc. There are **600** images per class, with **500** for training and **100** for testing. It is also a commonly benchmark for image classification and deep learning research.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "62eaac52-98d3-4c19-a421-8a96aa86fb85"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we load and plot the dataset that we will fine tune the model on."
   ],
   "id": "f1dc3e31-abd9-4a3e-a1c7-caa28b8cc63d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the CIFAR-100 dataset\n",
    "loader = get_res_loaders(dataset=\"cifar100\", batch_size=16)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "69a8b28c-d9ba-41e6-9e39-32223aa9a6eb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We fine-tune the model again for 10 epochs on the **CIFAR-100** dataset."
   ],
   "id": "209c4d08-0e2f-4a9f-828f-4d45326d34ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on CIFAR-100\n",
    "train_acc_cifar100, test_acc_cifar100 = train_res_model(loaders=loader)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "print_time(start_time, end_time)"
   ],
   "id": "670f9f9b-f73c-49ad-aff7-9f2af18182ac"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save the results to be able to use it later for creating this model’s results table."
   ],
   "id": "0e3370f4-f9c5-436e-8568-7de7be310658"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"cifar100\"] = test_acc_cifar100[-1]"
   ],
   "id": "a02a95d6-068a-4cdd-aa31-9b8e7109d63b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/resnet.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ],
   "id": "57fa3e2e-47bd-424f-952e-abf78a59af5f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "257ab54b-93ce-4ae8-9d08-ba3cfad59415"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oxford-IIIT Pets\n",
    "\n",
    "The Oxford-IIIT Pets is a **37** category pet dataset with roughly **200** images for each class created by the Visual Geometry Group at Oxford. The images have large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI (region of interest), and pixel level trimap segmentation. The dataset is useful for fine-grained image classification and segmentation tasks.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "110ca1fc-5880-4ac8-8098-ce11ac1961ec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start again by loading and plotting the dataset. Make sure the batch size written is suitable for the GPU you are using to prevent any errors."
   ],
   "id": "418d90ef-bdfa-4efc-8013-a9e97b5ce80c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the Oxford-IIIT Pets dataset\n",
    "loader = get_res_loaders(dataset=\"oxford_pets\", batch_size=16)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "b0853737-1a78-489e-bbfe-697fe22e07ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We fine-tune the model pretrained on `ImageNet-21k` on the `OxfordPets` dataset. We get the training and testing accuracies for 10 epochs in the `train_acc_oxford_pets` and `test_acc_oxford_pets` arrays."
   ],
   "id": "de6c5f7e-ecbc-4dc6-9487-040fe6214d01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on Oxford-IIIT Pets\n",
    "train_acc_oxford_pets, test_acc_oxford_pets = train_res_model(loaders=loader, lr=0.0001)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "print_time(start_time, end_time)"
   ],
   "id": "f5da924b-5fdf-4f8f-ae02-53b0a57f5bfd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save the results in the runs dictionary under the dataset name."
   ],
   "id": "a687406b-7109-41d6-88bd-4ab13dc060de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"oxford_pets\"] = test_acc_oxford_pets[-1]"
   ],
   "id": "31c458c1-727b-4da9-8c22-2b01762f48c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/resnet.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ],
   "id": "da29f1de-2640-4152-8a55-4cc5866b6ece"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "77debc6c-1706-4396-ab3e-8908e546173e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oxford Flowers-102\n",
    "\n",
    "The Oxford Flowers-102 dataset consists of **102** flower categories commonly occurring in the United Kingdom. Each class consists of between **40 and 258** images. The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories. The dataset also provides image labels, segmentations, and distances based on shape and color features.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "27dd20b9-19a7-4fb6-9a76-0a57c403bcea"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch dataset for this dataset does not contain the class labels, so we create an array called `flower_classes` to store them. We then use dataloaders to load the dataset and display the first 10 images from the train loader."
   ],
   "id": "9d933ba8-b5a4-45f2-b61b-7cdc6222e247"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the Oxford Flowers-102 Pets dataset\n",
    "loader = get_res_loaders(dataset=\"flowers_102\", batch_size=16)\n",
    "\n",
    "# We initialize the flowers names as they are not on Pytorch (used for plotting)\n",
    "flower_classes = ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',\n",
    " 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle',\n",
    " 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower',\n",
    " 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary',\n",
    " 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke',\n",
    " 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly',\n",
    " 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy',\n",
    " 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy',\n",
    " 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura',\n",
    " 'geranium', 'orange dahlia', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan',\n",
    " 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy',\n",
    " 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily',\n",
    " 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia',\n",
    " 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea',\n",
    " 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']\n",
    "\n",
    "# Save the Class names in the dataset\n",
    "loader[\"train_loader\"].dataset.classes = flower_classes\n",
    "# Plot dataset\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "0ad4a030-4fa8-449f-9565-6c6640464858"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We fine-tune the model on the dataset and obtain the train and test accuracies array. We change the number of epochs to 14 for this dataset as the size of the dataset is smaller than the other datasets. We also change the learning to `0.0001` as the fine-tuning yields better results with this learning rate."
   ],
   "id": "e3c9fde8-c0f5-48c3-bedd-5da07a8dc434"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on Oxford Flowers-102\n",
    "train_acc_oxford_flowers, test_acc_oxford_flowers = train_res_model(loader, epochs=14, lr=0.0001)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "print_time(start_time, end_time)"
   ],
   "id": "461b9a79-f606-455b-80d5-3d116e38211a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We store the result in `runs` dictionary to be used later for creating the table."
   ],
   "id": "3f32ac7a-945d-472c-89f1-f9091e74bfbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "runs[\"oxford_flowers\"] = test_acc_oxford_flowers[-1]"
   ],
   "id": "b8bc4a48-73dd-4dea-a11a-092d62dab0f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/resnet.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ],
   "id": "b5ebfd4d-8cd8-4c1e-8082-0971ae0143f3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Now that we are done with the **ResNet** model which we consider our baseline, we can start fine-tuning the the **ViT** model."
   ],
   "id": "2d1be3c7-ba84-4e3d-93ed-d27bb8e0d943"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
