{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the ResNet model\n",
    "\n",
    "In this notebook, we will explore how to fine-tune a pre-trained model for image classification tasks. We will use the **ResNet-152x4** model, which is a deep convolutional neural network with *152* layers and *4* times more channels than the standard **ResNet-152**. This model was trained on the **ImageNet-21k** dataset, which is a large-scale dataset with about *14 million images and 21,000 classes*. We will fine-tune this model on different datasets that are commonly used for image classification, such as **ImageNet-1k**, **CIFAR-10**, **CIFAR-100**, **Oxford Pets** and **Oxford flowers**. We will save the results to compare them with the **ViT** later.\n",
    "\n",
    "This notebook contains five datasets, you have to run the Utility Functions subsection then you can choose which datasets you want to evaluate the BiT model on by running its specific subsection. The following table contains the approximate running time for each dataset:\n",
    "\n",
    "|    Dataset     | Time (hh:mm:ss) |\n",
    "|:--------------:|:---------------:|\n",
    "|    Imagenet    |    02:15:00     |\n",
    "|    CIFAR-10    |    07:35:00     |\n",
    "|   CIFAR-100    |    07:35:00     |\n",
    "|  Oxford Pets   |    00:50:00     |\n",
    "| Oxford Flowers |    00:40:00     |\n",
    "\n",
    "**üö® Note that this notebook will not run using the ResNet-152x4 model on colab free version üö®**\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "f35d0e6a-0194-418b-8052-7b24b494ce4f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "The next few cells show the functions used for fine-tuning the **ResNet-152x4** model. We start by importing the required modules:"
   ],
   "id": "995b92c7-1362-4b7f-8037-cf5ae9f97f4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms, datasets, models"
   ],
   "id": "c472bdcc-4bec-45e1-8661-f97b978c0151"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The function `get_res_loaders` is a helper function that can create the dataloaders for *training* and *testing* the **ResNet** model on different image classification datasets. The function takes two arguments: `dataset` and `batch_size`. The `dataset` argument is a string that specifies the name of the dataset to use, such as ‚Äúimagenet‚Äù, ‚Äúcifar10‚Äù, ‚Äúcifar100‚Äù, ‚Äúoxford_pets‚Äù, or ‚Äúflowers_102‚Äù. The `batch_size` argument is an integer that specifies the number of images to feed to the model at each iteration. We can also adjust the `batch_size` argument according to the GPU we have. The function returns three dataloaders: one for the training data, one for the development data and one for the testing data. The development splits are as the following: 10% for Pets and Flowers, 2% for CIFAR.\n",
    "\n",
    "The function uses a dictionary called `known_dataset_sizes` to store the image sizes for each dataset. It also uses a transformation function called `normalize_transform` to normalize the images using the channel means. The function then creates two different transformation functions for the training and testing data, using the PyTorch transforms module.\n",
    "\n",
    "**üåÄ Reproducibility Challenge: Image resolution**  \n",
    "One challenge that we face in this notebook was the discrepancy between the image resolution used in the paper and the code. The paper reported using a resolution of **384x384** pixels for the ResNet models, but the code provided different resolutions for different datasets. In the code they used **128x128** for CIFAR datasets, **224x224** for Oxford datasets and **480x480** for ImageNet-1k dataset. To determine the best resolution, we would need to try all four resolutions, which is computationally expensive. However, this step was completed beforehand and it was found that the resolutions in `known_dataset_sizes` yield good results but may not be the optimal ones.\n",
    "\n",
    "**Can you explain how using different resolutions affects computation and what other hyperparameters it may impact? ü§î**\n",
    "\n",
    "**‚öôÔ∏è Computation Challenge: Batch Size**  \n",
    "The batch size used by the authors was **512** samples per batch which is considered huge, given that a 24GB GPU can hold the model and a maximum batch size of **16** samples for images with 128x128 resolution. Therefore, we can only use batch sizes that are compatible with our GPU memory."
   ],
   "id": "7e7085cd-3f38-410a-91c1-ceb5de4dc26b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_dataset_sizes = {\n",
    "  'cifar10': (128, 128),\n",
    "  'cifar100': (128, 128),\n",
    "  'oxford_pets': (128, 128),\n",
    "  'flowers_102': (128, 128),\n",
    "  'imagenet': (384, 384),\n",
    "}\n",
    "\n",
    "def get_res_loaders(dataset=\"imagenet\", batch_size=64):\n",
    "    \"\"\"\n",
    "    This loads the whole dataset into memory and returns train and test data to\n",
    "    be used by the ResNet model\n",
    "    @param dataset (string): dataset name to load\n",
    "    @param batch_size (int): batch size for training and testing\n",
    "\n",
    "    @returns dict() with train and test data loaders with keys `train_loader`, `test_loader`\n",
    "    \"\"\"\n",
    "    # Get image size\n",
    "    crop = known_dataset_sizes[dataset]\n",
    "    precrop = (160,160)\n",
    "    # Normalization using channel means\n",
    "    normalize_transform = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "    # Creating transform function\n",
    "    train_transform = transforms.Compose([\n",
    "                        transforms.Resize(precrop),\n",
    "                        transforms.RandomCrop(crop),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ToTensor(),\n",
    "                        normalize_transform,\n",
    "                    ])\n",
    "\n",
    "    # Test transformation function\n",
    "    test_transform =transforms.Compose([transforms.Resize(crop), transforms.ToTensor(), normalize_transform])\n",
    "\n",
    "    # Loader arguments\n",
    "    loader_args = {\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "\n",
    "     # Load the dataset from torchvision datasets\n",
    "    if dataset == \"imagenet\":\n",
    "        # Load the dataset\n",
    "        original_test_dataset = datasets.ImageFolder(root=os.path.join('data', 'imagenet', 'val'), transform=test_transform)\n",
    "        # Create Data Loader\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dataset=original_test_dataset,\n",
    "            shuffle=True,\n",
    "            **loader_args)\n",
    "        # Return Test Loader\n",
    "        return {\"test_loader\": test_loader}\n",
    "\n",
    "    elif dataset == \"cifar10\":\n",
    "        # Load CIFAR-10\n",
    "        original_train_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == \"cifar100\":\n",
    "        # Load CIFAR-100\n",
    "        original_train_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == \"oxford_pets\":\n",
    "        # Load Oxford-IIIT Pets\n",
    "        original_train_dataset = datasets.OxfordIIITPet(root=os.path.join('data', 'oxford_iiit_pets_data'),\n",
    "                                             split='trainval', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.OxfordIIITPet(root=os.path.join('data', 'oxford_iiit_pets_data'),\n",
    "                                             split='test', transform=test_transform, download=True)\n",
    "    elif dataset == \"flowers_102\":\n",
    "        # Load Oxford Flowers-102\n",
    "        original_train_dataset = datasets.Flowers102(root=os.path.join('data', 'oxford_flowers_102_data'),\n",
    "                                             split='train', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.Flowers102(root=os.path.join('data', 'oxford_flowers_102_data'),\n",
    "                                             split='test', transform=test_transform, download=True)\n",
    "    else:\n",
    "        # Raise an error if the dataset is not valid\n",
    "        raise ValueError(\"Invalid dataset name. Please choose one of the following: imagenet, cifar10, cifar100, oxford_pets, flowers_102\")\n",
    "\n",
    "    \n",
    "    # Set the validation set size\n",
    "    val_size = int(0.02 * len(original_train_dataset)) if dataset in ['cifar10', 'cifar100'] else int(0.1 * len(original_train_dataset))\n",
    "    \n",
    "    # Set train size as remaining data\n",
    "    train_size = len(original_train_dataset) - val_size\n",
    "\n",
    "    # Split the original train dataset into train and validation datasets\n",
    "    train_dataset, val_dataset = random_split(original_train_dataset, [train_size, val_size])\n",
    "\n",
    "    # Create the data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_test_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    return {\"train_loader\": train_loader,\n",
    "            \"val_loader\": val_loader,\n",
    "            \"test_loader\": test_loader}"
   ],
   "id": "19fde5f7-c605-4d91-9e1f-c7178f7232ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Then we use the **ResNet** implementation from the [official repository](https://github.com/google-research/big_transfer/blob/master/bit_pytorch/models.py) to create our model."
   ],
   "id": "5ae43d27-0c59-4904-a2d0-5ddf0876da54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StdConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    A custom convolutional layer that standardizes the weights before applying them.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        w = self.weight\n",
    "        # Compute the variance and mean of the weights along the channel, height and width dimensions\n",
    "        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
    "        # Standardize the weights by subtracting the mean and dividing by the standard deviation\n",
    "        w = (w - m) / torch.sqrt(v + 1e-10)\n",
    "        # Apply the standardized weights\n",
    "        return F.conv2d(x, w, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "# helper function to create a 3x3 convolutional layer with standardization.    \n",
    "def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=3, stride=stride, padding=1, bias=bias, groups=groups)\n",
    "\n",
    "# helper function to create a 1x1 convolutional layer with standardization.\n",
    "def conv1x1(cin, cout, stride=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=1, stride=stride, padding=0, bias=bias)\n",
    "\n",
    "# helper function to convert the weight format from TensorFlow (HWIO) to PyTorch (OIHW).\n",
    "def tf2th(conv_weights):\n",
    "    if conv_weights.ndim == 4:\n",
    "        # Transpose the dimensions from height-width-input-output to output-input-height-width\n",
    "        conv_weights = np.transpose(conv_weights, [3, 2, 0, 1])\n",
    "    return torch.from_numpy(conv_weights)\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom residual block that uses pre-activation and group normalization.\n",
    "    This is based on the paper \"Identity Mappings in Deep Residual Networks\" by Kaiming He et al.\n",
    "    https://arxiv.org/abs/1603.05027\n",
    "    However, this implementation differs from the original one by putting\n",
    "    the stride on the 3x3 convolution instead of the 1x1 convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, cin, cout=None, cmid=None, stride=1):\n",
    "        super().__init__()\n",
    "        # Set the output channels to be the same as the input channels if not specified\n",
    "        cout = cout or cin\n",
    "        # Set the middle channels to be one fourth of the output channels if not specified\n",
    "        cmid = cmid or cout//4\n",
    "\n",
    "        # Define the group normalization and convolution layers for the block\n",
    "        self.gn1 = nn.GroupNorm(32, cin)\n",
    "        self.conv1 = conv1x1(cin, cmid)\n",
    "        self.gn2 = nn.GroupNorm(32, cmid)\n",
    "        self.conv2 = conv3x3(cmid, cmid, stride)  # Original ResNetv2 has it on conv1!!\n",
    "        self.gn3 = nn.GroupNorm(32, cmid)\n",
    "        self.conv3 = conv1x1(cmid, cout)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Define the projection layer for the residual branch if needed\n",
    "        if (stride != 1 or cin != cout):\n",
    "            self.downsample = conv1x1(cin, cout, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv'ed branch\n",
    "        out = self.relu(self.gn1(x))\n",
    "\n",
    "        # Residual branch\n",
    "        residual = x\n",
    "        # If there is a projection layer, apply it to the output of the first activation\n",
    "        if hasattr(self, 'downsample'):\n",
    "            residual = self.downsample(out)\n",
    "\n",
    "        # The first block has already applied pre-act before splitting, see Appendix.\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(self.relu(self.gn2(out)))\n",
    "        out = self.conv3(self.relu(self.gn3(out)))\n",
    "        \n",
    "        # Add the residual branch to the conv'ed branch and return\n",
    "        return out + residual\n",
    "    \n",
    "    # helper function to load the weights from a TensorFlow model\n",
    "    def load_from(self, weights, prefix=''):\n",
    "        with torch.no_grad():\n",
    "            # Copy the weights for each layer from the TensorFlow model to the PyTorch model\n",
    "            self.conv1.weight.copy_(tf2th(weights[prefix + 'a/standardized_conv2d/kernel']))\n",
    "            self.conv2.weight.copy_(tf2th(weights[prefix + 'b/standardized_conv2d/kernel']))\n",
    "            self.conv3.weight.copy_(tf2th(weights[prefix + 'c/standardized_conv2d/kernel']))\n",
    "            self.gn1.weight.copy_(tf2th(weights[prefix + 'a/group_norm/gamma']))\n",
    "            self.gn2.weight.copy_(tf2th(weights[prefix + 'b/group_norm/gamma']))\n",
    "            self.gn3.weight.copy_(tf2th(weights[prefix + 'c/group_norm/gamma']))\n",
    "            self.gn1.bias.copy_(tf2th(weights[prefix + 'a/group_norm/beta']))\n",
    "            self.gn2.bias.copy_(tf2th(weights[prefix + 'b/group_norm/beta']))\n",
    "            self.gn3.bias.copy_(tf2th(weights[prefix + 'c/group_norm/beta']))\n",
    "            # If there is a projection layer, copy its weights as well\n",
    "            if hasattr(self, 'downsample'):\n",
    "                self.downsample.weight.copy_(tf2th(weights[prefix + 'a/proj/standardized_conv2d/kernel']))\n",
    "        \n",
    "        # Return the PyTorch model with loaded weights\n",
    "        return self\n",
    "\n",
    "# ResNet Class\n",
    "class ResNetV2(nn.Module):\n",
    "    BLOCK_UNITS = {\n",
    "        'r50': [3, 4, 6, 3],\n",
    "        'r101': [3, 4, 23, 3],\n",
    "        'r152': [3, 8, 36, 3],\n",
    "    }\n",
    "\n",
    "    def __init__(self, block_units, width_factor, head_size=21843, zero_head=False):\n",
    "        super().__init__()\n",
    "        wf = width_factor  # shortcut 'cause we'll use it a lot.\n",
    "\n",
    "        self.root = nn.Sequential(OrderedDict([\n",
    "            ('conv', StdConv2d(3, 64*wf, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            ('padp', nn.ConstantPad2d(1, 0)),\n",
    "            ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0)),\n",
    "            # The following is subtly not the same!\n",
    "            #('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        self.body = nn.Sequential(OrderedDict([\n",
    "            ('block1', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin= 64*wf, cout=256*wf, cmid=64*wf))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=256*wf, cout=256*wf, cmid=64*wf)) for i in range(2, block_units[0] + 1)],\n",
    "            ))),\n",
    "            ('block2', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin=256*wf, cout=512*wf, cmid=128*wf, stride=2))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=512*wf, cout=512*wf, cmid=128*wf)) for i in range(2, block_units[1] + 1)],\n",
    "            ))),\n",
    "            ('block3', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin= 512*wf, cout=1024*wf, cmid=256*wf, stride=2))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=1024*wf, cout=1024*wf, cmid=256*wf)) for i in range(2, block_units[2] + 1)],\n",
    "            ))),\n",
    "            ('block4', nn.Sequential(OrderedDict(\n",
    "                [('unit01', PreActBottleneck(cin=1024*wf, cout=2048*wf, cmid=512*wf, stride=2))] +\n",
    "                [(f'unit{i:02d}', PreActBottleneck(cin=2048*wf, cout=2048*wf, cmid=512*wf)) for i in range(2, block_units[3] + 1)],\n",
    "            ))),\n",
    "        ]))\n",
    "\n",
    "        self.zero_head = zero_head\n",
    "        self.head = nn.Sequential(OrderedDict([\n",
    "            ('gn', nn.GroupNorm(32, 2048*wf)),\n",
    "            ('relu', nn.ReLU(inplace=True)),\n",
    "            ('avg', nn.AdaptiveAvgPool2d(output_size=1)),\n",
    "            ('conv', nn.Conv2d(2048*wf, head_size, kernel_size=1, bias=True)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.head(self.body(self.root(x)))\n",
    "        assert x.shape[-2:] == (1, 1)  # We should have no spatial shape left.\n",
    "        return x[...,0,0]\n",
    "\n",
    "    def load_from(self, weights, prefix='resnet/'):\n",
    "        with torch.no_grad():\n",
    "            self.root.conv.weight.copy_(tf2th(weights[f'{prefix}root_block/standardized_conv2d/kernel']))\n",
    "            self.head.gn.weight.copy_(tf2th(weights[f'{prefix}group_norm/gamma']))\n",
    "            self.head.gn.bias.copy_(tf2th(weights[f'{prefix}group_norm/beta']))\n",
    "            if self.zero_head:\n",
    "                nn.init.zeros_(self.head.conv.weight)\n",
    "                nn.init.zeros_(self.head.conv.bias)\n",
    "            else:\n",
    "                self.head.conv.weight.copy_(tf2th(weights[f'{prefix}head/conv2d/kernel']))\n",
    "                self.head.conv.bias.copy_(tf2th(weights[f'{prefix}head/conv2d/bias']))\n",
    "\n",
    "            for bname, block in self.body.named_children():\n",
    "                for uname, unit in block.named_children():\n",
    "                    unit.load_from(weights, prefix=f'{prefix}{bname}/{uname}/')\n",
    "        return self"
   ],
   "id": "c8146e79-3880-446a-a9e4-5c6126eacbcd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following are two helper functions that we use for calculating and evaluating the model performance:\n",
    "\n",
    "-   `get_accuracy`: This function takes the model predictions and the true labels as inputs and returns the accuracy as a float value.\n",
    "-   `evaluate_on_test`: This function takes the model, the test dataloader, and the device as inputs and returns the test accuracy of the model on the given test set."
   ],
   "id": "465f96fb-20fc-46d8-b14f-61bf7ba0cc2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes predictions and true values to return accuracies\n",
    "def get_accuracy(logit, true_y):\n",
    "    pred_y = torch.argmax(logit, dim=1)\n",
    "    return (pred_y == true_y).float().mean()\n",
    "\n",
    "# This Function is used to evaluate the model\n",
    "def evaluate_on_test(model, test_loader, device=\"cpu\"):\n",
    "    # Evaluate the model on all the test batches\n",
    "    accuracies = []\n",
    "    model.eval()\n",
    "    for batch_idx, (data_x, data_y) in enumerate(test_loader):\n",
    "        data_x = data_x.to(device)\n",
    "        data_y = data_y.to(device)\n",
    "\n",
    "        model_y = model(data_x)\n",
    "        batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "        accuracies.append(batch_accuracy.item())\n",
    "\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print(f\"Mean accuracy at batch: {batch_idx} is {np.mean(accuracies) * 100}\")\n",
    "\n",
    "    test_accuracy = np.mean(accuracies) * 100\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "    return test_accuracy"
   ],
   "id": "30abd86d-4368-4b16-ac19-e95785dc2a6f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The function `train_res_model` takes the following arguments and returns the train and test accuracies of the model:\n",
    "\n",
    "-   `loaders`: A dictionary of PyTorch dataloaders for the train and test sets.\n",
    "-   `model_name`: A string to specify the name of the ResNet model to use. The default is ‚Äòr152‚Äô, which corresponds to ResNet-152.\n",
    "-   `bit_model`: A string to specify the name of the pre-trained Big Transfer (BiT) model to use. The default is ‚ÄúBiT-M-R152x4‚Äù, which corresponds to BiT-M with ResNet-152 backbone and width factor 4.\n",
    "-   `width_factor`: An integer to scale the width of the ResNet model. The default is 4, which means the number of channels in each layer is multiplied by 4.\n",
    "-   `lr`: A float to set the learning rate for the optimizer. The default is 0.0003.\n",
    "-   `epochs`: An integer to set the number of epochs for the training loop. The default is 10.\n",
    "\n",
    "The function first creates a ResNet model with the specified arguments and loads the weights from the pre-trained BiT model. Then, it creates an optimizer and a scheduler for the training process. Finally, it runs a training loop for the given number of epochs, where it computes the loss and accuracy for each batch of data, updates the model parameters, and adjusts the learning rate. Then it passes the model and the test loader to the `evaluate_on_test` function and then returns the test accuracy\n",
    "\n",
    "**üåÄ Reproducibility Challenge: learning rate**  \n",
    "The second reproducibility challenge that we encountered in this notebook was the lack of information about the optimal learning rate for each dataset. The paper only reported the learning rates used for the grid search, but not the ones selected for the final experiments. As a result, we assumed that the learning rates we use for each dataset would yield good results, but not necessarily the best.\n",
    "\n",
    "**Based on the running time table provided above, approximately how long would it take to run the grid search for the CIFAR-10 dataset? üßê**\n",
    "\n",
    "**‚öôÔ∏è Computation Challenge: number of epochs**  \n",
    "Another challenge that we faced in this notebook was the ambiguity of the number of epochs used for fine-tuning. The paper did not specify the exact number of epochs, but only the number of steps relative to a batch size of 512 (which are huge). However, in some parts of the paper, the results were reported based on epochs. This made it difficult to compare our results with the paper‚Äôs results. Therefore, we decided to experiment with different numbers of epochs to find the best one for each dataset.\n",
    "\n",
    "**Based on the table provided in the Experiments notebook, how many epochs would be equivalent to the number of steps used by the authors on the CIFAR-100 dataset (50,000 training samples) if they used a batch size of 512 as stated in the paper? And how much time would this take? ü§î**"
   ],
   "id": "a7271782-a986-49b6-be3c-3070cdf6f6af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model and return train and test accuracies\n",
    "def train_res_model(loaders, title='', model_name='r152', bit_model=\"BiT-M-R152x4\",\n",
    "                       width_factor=4, lr=0.0003, epochs=10, random_seed=42, save=False):\n",
    "\n",
    "    # Create experiment directory name if none\n",
    "    experiment_dir = os.path.join('experiments', title)\n",
    "\n",
    "    # make experiment directory\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "    # Set the seed\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Get num_classes\n",
    "    num_classes = len(loaders[\"train_loader\"].dataset.dataset.classes)\n",
    "\n",
    "    # Get weights\n",
    "    print(f\"Loading {bit_model} weights...\")\n",
    "    weights = get_weights(bit_model)\n",
    "    print(\"Weight successfully loaded\")\n",
    "\n",
    "    # Initialize the ResNet model\n",
    "    model = ResNetV2(ResNetV2.BLOCK_UNITS[model_name], width_factor=width_factor, head_size=num_classes, zero_head=True)\n",
    "    model.load_from(weights)\n",
    "    model.to(device);\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Calculate per_step, authors used 512 batch size\n",
    "    batch_size = loaders[\"train_loader\"].batch_size\n",
    "    T_max = len(loaders[\"train_loader\"]) * epochs\n",
    "\n",
    "    # Create the scheduler with cosine learning rate decay\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Make model params trainable\n",
    "        model.to(device);\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "\n",
    "        # Calculate loss and gradients for models on every training batch\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"train_loader\"]):\n",
    "            data_x = data_x.to(device)\n",
    "            data_y = data_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model_y = model(data_x)\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "            # Perform back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # Store training accuracy for plotting\n",
    "        train_loss = np.mean(losses)\n",
    "        train_accuracy = np.mean(accuracies)*100\n",
    "        print(\"Train accuracy: {} Train loss: {}\".format(train_accuracy, train_loss))\n",
    "\n",
    "        # Evaluate the model on all the test batches\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        # Move the model to CPU\n",
    "        model.to(\"cpu\")\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"val_loader\"]):\n",
    "            # Move the data to CPU\n",
    "            data_x = data_x.to(\"cpu\")\n",
    "            data_y = data_y.to(\"cpu\")\n",
    "\n",
    "            model_y = model(data_x)\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Store test accuracy for plotting\n",
    "        val_loss = np.mean(losses)\n",
    "        val_accuracy = np.mean(accuracies)*100\n",
    "        print(\"Validation accuracy: {} Validation loss: {}\".format(val_accuracy, val_loss))\n",
    "\n",
    "    if save:\n",
    "        # Save the final model\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, os.path.join(experiment_dir, f'{bit_model}.pt'))\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_acc = evaluate_on_test(model, loaders['test_loader'])\n",
    "\n",
    "    # Delete the data and model outputs from GPU memory\n",
    "    del model\n",
    "    # Release unused memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # return the accuracies\n",
    "    return test_acc"
   ],
   "id": "5d4b881f-4bcc-4ef5-8c03-1ee104260b80"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following function `plot_images_from_dataloader` takes a PyTorch dataloader as an input and plots 10 of the images from the first batch of data. The function also shows the labels of the images according to the classes attribute of the dataloader‚Äôs dataset"
   ],
   "id": "b1a94dfc-6da9-48d4-a0c8-96535de30579"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot 10 of the images\n",
    "def plot_images_from_dataloader(dataloader, classes=None):\n",
    "    # Initialize empty tensors for images and labels\n",
    "    images = torch.empty(0)\n",
    "    labels = torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    # Loop until the images and labels have at least 10 elements\n",
    "    while len(images) < 10:\n",
    "        # Get the next batch of images and labels from the dataloader\n",
    "        batch_images, batch_labels = next(iter(dataloader))\n",
    "        # Concatenate the batch images and labels to the existing tensors\n",
    "        images = torch.cat((images, batch_images), dim=0)\n",
    "        labels = torch.cat((labels, batch_labels), dim=0)\n",
    "    # Get class names\n",
    "    if classes is None:\n",
    "        classes = dataloader.dataset.dataset.classes\n",
    "    # Create a figure with 2 rows and 5 columns\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = images[i]\n",
    "        label = classes[labels[i]]\n",
    "        # Unnormalize the image\n",
    "        image = image / 2 + 0.5\n",
    "        image = image.numpy()\n",
    "        # Transpose the image\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        # Plot the image on the axis\n",
    "        ax.imshow(image)\n",
    "        # Set title as label\n",
    "        ax.set_title(label)\n",
    "        # Turn off the axis ticks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()"
   ],
   "id": "bd7f52e8-dc6e-4609-ab58-427d6493d9aa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following cell contains function to format time and print it, while the other function is used to load the downloaded model weights."
   ],
   "id": "889a82f1-a1cd-42f4-877f-904f1aee4ae4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to load npz files\n",
    "def get_weights(bit_variant):\n",
    "    return np.load(f'pretrained_models/{bit_variant}.npz')\n",
    "\n",
    "# Function used to calculated time\n",
    "def print_time(start_time, end_time):\n",
    "    # Calculate the difference in seconds\n",
    "    diff = end_time - start_time\n",
    "\n",
    "    # Convert the difference to hours, minutes, and seconds\n",
    "    hours, remainder = divmod(diff, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "    # Create time in hours:minutes:seconds format\n",
    "    time_string = f\"{int(hours)}:{int(minutes)}:{seconds}\"\n",
    "\n",
    "    # Print the time\n",
    "    print(f\"Cell execution time: {time}\")\n",
    "\n",
    "    return time_string"
   ],
   "id": "305d62a3-38c9-4d06-8d1d-d942f0ac21c4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We download the models that we will be using through the rest of the notebook:\n",
    "\n",
    "-   The `BiT-M-R152x4` is pre-trained on the **ImageNet-21k** and not fine tuned at all.\n",
    "-   The `BiT-M-R152x4-ILSVRC2012` is pre-trained on the **ImageNet-21k** and fine tuned on the **ImageNet-1k**"
   ],
   "id": "512dd8a3-c035-43e8-9037-30c412c389bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to hold the pre-trained models\n",
    "!mkdir -p pretrained_models\n",
    "\n",
    "# Download the ResNet152x4 model\n",
    "![ -e pretrained_models/BiT-M-R152x4.npz ] || \\\n",
    "curl -L -o pretrained_models/BiT-M-R152x4.npz \"https://storage.googleapis.com/bit_models/BiT-M-R152x4.npz\"\n",
    "\n",
    "# Download the ResNet152x4 model fine tuned on ImageNet\n",
    "![ -e pretrained_models/BiT-M-R152x4-ILSVRC2012.npz ] || \\\n",
    "curl -L -o pretrained_models/BiT-M-R152x4-ILSVRC2012.npz \"https://storage.googleapis.com/bit_models/BiT-M-R152x4-ILSVRC2012.npz\""
   ],
   "id": "d3921871-12d3-46cd-bda0-48094bda1a18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "84c6a02e-34ec-4473-a28b-6b13fbf0843d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚öôÔ∏è Computation Challenge: High GPU memory requirement**  \n",
    "After installing the models from the previous cell, you can see that the model itself is 4GB which means it will take a huge space in the GPU. This actually challenging as you will want to optimize the GPU RAM without crashing it.\n",
    "\n",
    "The batch size chosen in the following runs are for a `RTX6000 GPU`. If you are using another GPU you can try changing it to maximize the usage of your GPU.\n",
    "\n",
    "Note that maximizing GPU usage is just an option, we did this as authors used a very high batch size. you can try different batch sizes to find which batch size gives the best performance.\n",
    "\n",
    "You can use any of these commands in the terminal while running the cells to check how much of your GPU memory is utilized:\n",
    "\n",
    "-   `nvidia-smi --query-gpu=gpu_name,memory.used,memory.free,memory.total --format=csv`\n",
    "-   `nvidia-smi pmon -s m -c 1`\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "2b287aec-7575-498d-8ff6-77cacdab9c0e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet\n",
    "\n",
    "The ImageNet dataset consists of **1000** object classes and contains **1,281,167** training images, **50,000** validation images and **100,000** test images. The images vary in resolution but it is common practice to train deep learning models on sub-sampled images of **256x256** pixels. This dataset is widely used for image classification and localization tasks and has been the benchmark for many state-of-the-art algorithms.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "76fbf2d5-9ecb-45a5-8350-81623e9b2fb3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the dataset and choose the `batch_size` that suits our GPU capacity. This will help us avoid errors during the training process. Next, we use the `plot_images_from_dataloader` function to display 10 of the images from the first batch of data. We can see the labels of the images on top of each plot. However, we need to make sure that the batch size is at least 10, otherwise the function will raise an error."
   ],
   "id": "61728632-75a1-400a-b413-8b8e6b016020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the ImageNet dataset\n",
    "loader = get_res_loaders(dataset=\"imagenet\", batch_size=1)\n",
    "plot_images_from_dataloader(loader[\"test_loader\"], loader[\"test_loader\"].dataset.classes)"
   ],
   "id": "d9115d20-eb72-4de7-b70f-75400adcd805"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We can use the `BiT-M-R152x4-ILSVRC2012` model, which is already fine-tuned on the **ImageNet** dataset, to evaluate its performance on the test set. This model is provided by the authors of the paper, so we expect to reproduce their results exactly. We will create the model object, load its pre-trained weights, and run it on the test data."
   ],
   "id": "1641b6e8-3bcc-48e3-84c7-4e6d8d480b36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"CUDA Recognized\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Get num_classes\n",
    "num_classes = len(loader[\"test_loader\"].dataset.classes)\n",
    "\n",
    "# Get weights\n",
    "print(f\"Loading BiT-M-R152x4-ILSVRC2012 weights...\")\n",
    "weights = get_weights(\"BiT-M-R152x4-ILSVRC2012\")\n",
    "print(\"Weight successfully loaded\")\n",
    "\n",
    "# Initialize the ResNet model\n",
    "model = ResNetV2(ResNetV2.BLOCK_UNITS['r152'], width_factor=4, head_size=num_classes)\n",
    "model.load_from(weights)\n",
    "model.to(device);"
   ],
   "id": "cf9d30b7-906b-402e-a152-e247d53d084c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We use the `evaluate_on_test` function to get the test accuracy for the model. The validation data that we use for evaluation is of size 50k samples. This means that it is normal for the evaluation to take some time. We will print the time taken to evaluate the model and then delete the model to empty the GPU memory for the next dataset."
   ],
   "id": "fda2f469-bf5b-4f02-b909-9715d53b77b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save start time\n",
    "start_time = time.time()\n",
    "# Print the Performance of the Ready fine tuned model\n",
    "test_acc_imagenet = evaluate_on_test(model, loader[\"test_loader\"], device)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "imagenet_time = print_time(start_time, end_time)\n",
    "# delete model to free gpu\n",
    "del model\n",
    "# Release unused memory\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "2e86aab4-9d6b-42a1-a810-f4d1f3b0d11a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We store the model‚Äôs performance metrics in a dictionary, which we will use to create a table later.\n",
    "\n",
    "üõë If you want to try a different model, please remember to change the name of the json file where you save the dictionary, so that you do not **overwrite** your previous results."
   ],
   "id": "5f7862d8-eb8c-42dd-900e-f58f390c22a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"imagenet\"] =  test_acc_imagenet\n",
    "\n",
    "# Create dictionary times\n",
    "times = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet_time.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet_time.json\", \"r\") as f:\n",
    "        # Load the data from the file to times\n",
    "        times = json.load(f)\n",
    "\n",
    "# Add the time to a dictionary\n",
    "times[\"imagenet\"] =  imagenet_time"
   ],
   "id": "a64d79c6-a1a5-48c2-b841-79cf8dc29ca5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/resnet.json\", \"w\") as f:\n",
    "    json.dump(runs, f)\n",
    "\n",
    "with open(\"experiments/resnet_time.json\", \"w\") as f:\n",
    "    json.dump(times, f)"
   ],
   "id": "5a70fcf4-c040-44d3-b156-5c205805b766"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "a1339110-3502-457e-8b04-4c74150f5954"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of **60,000 32x32** color images in **10** different classes. The 10 classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. There are **6,000** images per class, with **5,000** for training and **1,000** for testing. It is a popular benchmark for image classification and deep learning research.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "c469e830-22e5-4281-9035-d5c0fd32c37b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the **CIFAR-10** dataset and some of the images in it. Notice that the *batch size* increased as the resolution of the image decreased."
   ],
   "id": "0d6d2555-37aa-4610-9896-3a037bb63d97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the CIFAR-10 dataset\n",
    "loader = get_res_loaders(dataset=\"cifar10\", batch_size=16)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "aa1fef1c-2e78-4812-9a75-d030bb79a59f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We then fine-tune the model for 10 epochs on the dataset and get the train and test accuracies. As mentioned above the dataset has 50k samples for training, that means it will take some time to run the below cell.\n",
    "\n",
    "**Were you able to guess the number of epochs correctly based on your answer above? ü§î**"
   ],
   "id": "f5f32a5d-87de-46bc-afec-eec19ede1f77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on CIFAR-10\n",
    "test_acc_cifar10 = train_res_model(loaders=loader)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "cifar10_time = print_time(start_time, end_time)"
   ],
   "id": "7f1da177-bb3d-434a-ab22-1af1ec4345f7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save the results in the same dictionary as before."
   ],
   "id": "cff46b90-db44-45a1-9919-1c2ae32fdac7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"cifar10\"] = test_acc_cifar10\n",
    "\n",
    "# Create dictionary times\n",
    "times = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet_time.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet_time.json\", \"r\") as f:\n",
    "        # Load the data from the file to times\n",
    "        times = json.load(f)\n",
    "\n",
    "# Add the time to a dictionary\n",
    "times[\"cifar10\"] =  cifar10_time"
   ],
   "id": "5c7e5f08-aae8-48af-9fbf-47bb5da60e49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/resnet.json\", \"w\") as f:\n",
    "    json.dump(runs, f)\n",
    "\n",
    "with open(\"experiments/resnet_time.json\", \"w\") as f:\n",
    "    json.dump(times, f)"
   ],
   "id": "08f98b68-ea74-4759-ade7-b139d226f83f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "27220f8b-ae2e-4b94-a83a-5c668e241d52"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-100\n",
    "\n",
    "The CIFAR-100 dataset consists of **60,000 32x32** color images in **100** different classes. The 100 classes are grouped into 20 superclasses, such as aquatic mammals, flowers, insects, vehicles, etc. There are **600** images per class, with **500** for training and **100** for testing. It is also a commonly benchmark for image classification and deep learning research.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "cb744572-608e-4891-be16-570a45d62645"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we load and plot the dataset that we will fine tune the model on."
   ],
   "id": "f8d5f15d-71d6-403d-9d2d-7173d19de4dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the CIFAR-100 dataset\n",
    "loader = get_res_loaders(dataset=\"cifar100\", batch_size=16)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "3cccc73c-28b5-4e56-bbba-ddf413a180d0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We fine-tune the model again for 10 epochs on the **CIFAR-100** dataset."
   ],
   "id": "3aa8cd8b-aeeb-4e36-a6d9-06689d581c28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on CIFAR-100\n",
    "test_acc_cifar100 = train_res_model(loaders=loader)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "cifar100_time = print_time(start_time, end_time)"
   ],
   "id": "f3106fd4-a2e4-4ae3-b150-7ef593c40911"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save the results to be able to use it later for creating this model‚Äôs results table."
   ],
   "id": "926f74c0-5a1c-4bcb-a2bc-e3bdb13bb999"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"cifar100\"] = test_acc_cifar100\n",
    "\n",
    "# Create dictionary times\n",
    "times = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet_time.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet_time.json\", \"r\") as f:\n",
    "        # Load the data from the file to times\n",
    "        times = json.load(f)\n",
    "\n",
    "# Add the time to a dictionary\n",
    "times[\"cifar100\"] =  cifar100_time"
   ],
   "id": "009b6672-4c49-492e-a375-50ac77b508e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/resnet.json\", \"w\") as f:\n",
    "    json.dump(runs, f)\n",
    "\n",
    "with open(\"experiments/resnet_time.json\", \"w\") as f:\n",
    "    json.dump(times, f)"
   ],
   "id": "b678acd6-b539-4f12-ba97-8fc92880d0ec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "ecbb0b7c-2fae-4f16-b4b8-07ea6ae56c26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oxford-IIIT Pets\n",
    "\n",
    "The Oxford-IIIT Pets is a **37** category pet dataset with roughly **200** images for each class created by the Visual Geometry Group at Oxford. The images have large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI (region of interest), and pixel level trimap segmentation. The dataset is useful for fine-grained image classification and segmentation tasks.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "2d8dc28e-0e81-4fe7-9202-9083ba4b7d4f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start again by loading and plotting the dataset. Make sure the batch size written is suitable for the GPU you are using to prevent any errors."
   ],
   "id": "561efbe0-ed00-4095-9221-cf7e1bd8fb11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the Oxford-IIIT Pets dataset\n",
    "loader = get_res_loaders(dataset=\"oxford_pets\", batch_size=16)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "a28c8555-01ed-47c8-acfc-b6c2f99a545d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We fine-tune the model pre-trained on `ImageNet-21k` on the `OxfordPets` dataset. We get the training and testing accuracies for 10 epochs in the `train_acc_oxford_pets` and `test_acc_oxford_pets` arrays."
   ],
   "id": "95b0d151-67a8-4fef-975f-71afd7580910"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on Oxford-IIIT Pets\n",
    "test_acc_oxford_pets = train_res_model(loaders=loader, lr=0.0001)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "oxford_pets_time = print_time(start_time, end_time)"
   ],
   "id": "a32fc023-eb19-4813-90e3-4e0a3ece3854"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save the results in the runs dictionary under the dataset name."
   ],
   "id": "e63b4d00-87d9-4a1d-834b-f48e77a3437f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "# Add the results to a dictionary\n",
    "runs[\"oxford_pets\"] = test_acc_oxford_pets\n",
    "\n",
    "# Create dictionary times\n",
    "times = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet_time.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet_time.json\", \"r\") as f:\n",
    "        # Load the data from the file to times\n",
    "        times = json.load(f)\n",
    "\n",
    "# Add the time to a dictionary\n",
    "times[\"oxford_pets\"] =  oxford_pets_time"
   ],
   "id": "1995683c-3354-4156-b23d-940b73c1c67c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/resnet.json\", \"w\") as f:\n",
    "    json.dump(runs, f)\n",
    "\n",
    "with open(\"experiments/resnet_time.json\", \"w\") as f:\n",
    "    json.dump(times, f)"
   ],
   "id": "9c897012-0938-4eb1-8c2b-7ca5f3f59eec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "1d2f2b20-969d-4bba-80e1-8107585c55a8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oxford Flowers-102\n",
    "\n",
    "The Oxford Flowers-102 dataset consists of **102** flower categories commonly occurring in the United Kingdom. Each class consists of between **40 and 258** images. The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories. The dataset also provides image labels, segmentations, and distances based on shape and color features.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "8158efcc-e92a-4dfa-b629-bcf7b4ecfa64"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch dataset for this dataset does not contain the class labels, so we create an array called `flower_classes` to store them. We then use dataloaders to load the dataset and display the first 10 images from the train loader."
   ],
   "id": "1f323110-b609-43bc-ac4c-c00d1a036617"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the Oxford Flowers-102 Pets dataset\n",
    "loader = get_res_loaders(dataset=\"flowers_102\", batch_size=16)\n",
    "\n",
    "# We initialize the flowers names as they are not on Pytorch (used for plotting)\n",
    "flower_classes = ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',\n",
    " 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle',\n",
    " 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower',\n",
    " 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary',\n",
    " 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke',\n",
    " 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly',\n",
    " 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy',\n",
    " 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy',\n",
    " 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura',\n",
    " 'geranium', 'orange dahlia', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan',\n",
    " 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy',\n",
    " 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily',\n",
    " 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia',\n",
    " 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea',\n",
    " 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']\n",
    "\n",
    "# Save the Class names in the dataset\n",
    "loader[\"train_loader\"].dataset.dataset.classes = flower_classes\n",
    "# Plot dataset\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "148bbb07-5b53-4159-903c-7e8a4f9ec3ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We fine-tune the model on the dataset and obtain the train and test accuracies array. We change the number of epochs to 14 for this dataset as the size of the dataset is smaller than the other datasets. We also change the learning to `0.0001` as the fine-tuning yields better results with this learning rate."
   ],
   "id": "0cc962bc-f36a-4f49-aa3e-3113059756d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Fine tune the model on Oxford Flowers-102\n",
    "test_acc_oxford_flowers = train_res_model(loader, epochs=14, lr=0.0001)\n",
    "# Calculate and print cell execution time\n",
    "end_time = time.time()\n",
    "oxford_flowers_time = print_time(start_time, end_time)"
   ],
   "id": "db62d1d5-786c-42a0-aa32-a2e2d74de0cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We store the result in `runs` dictionary to be used later for creating the table."
   ],
   "id": "5efeb9e4-dfb4-468a-a1a0-cafc761c030a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary runs\n",
    "runs = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet.json\", \"r\") as f:\n",
    "        # Load the data from the file to runs\n",
    "        runs = json.load(f)\n",
    "\n",
    "runs[\"oxford_flowers\"] = test_acc_oxford_flowers\n",
    "\n",
    "# Create dictionary times\n",
    "times = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(\"experiments/resnet_time.json\"):\n",
    "    # Open the file in read mode\n",
    "    with open(\"experiments/resnet_time.json\", \"r\") as f:\n",
    "        # Load the data from the file to times\n",
    "        times = json.load(f)\n",
    "\n",
    "# Add the time to a dictionary\n",
    "times[\"oxford_flowers\"] =  oxford_flowers_time"
   ],
   "id": "845cf901-ee14-4c66-898b-c163fb884649"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/resnet.json\", \"w\") as f:\n",
    "    json.dump(runs, f)\n",
    "\n",
    "with open(\"experiments/resnet_time.json\", \"w\") as f:\n",
    "    json.dump(times, f)"
   ],
   "id": "2f7d8553-aa3c-426b-9053-4734d0b3dbf9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "**Can you explain the differences between our approach to fine-tuning the model and the approach described in the paper? üßê**\n",
    "\n",
    "You don‚Äôt need to check the paper as everything they did was explained in this notebook and the table provided in the experiments notebook."
   ],
   "id": "ba4c9226-b17c-49ac-a858-d34953225107"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Although the authors did not provide results for the fine-tuned **ResNet** model pre-trained on the **ImageNet-21k** dataset in this paper, we can compare our results to their results for the model pre-trained on the proprietary **JFT-300M** dataset. Please fill in the following table template:\n",
    "\n",
    "|       Model        | Imagenet | CIFAR-10 | CIFAR-100 | Oxford Pets | Oxford Flowers |\n",
    "|:------------------:|:-------:|:-------:|:--------:|:----------:|:-------------:|\n",
    "| Authors‚Äô BiT (JFT) |          |          |           |             |                |\n",
    "|   Our BiT (I21k)   |          |          |           |             |                |\n",
    "|     Difference     |          |          |           |             |                |\n",
    "\n",
    "**Given the difference in the size of the pre-training datasets, do these results seem reasonable to you? In your opinion, are these results satisfactory, or could we have tried something else to achieve better results? ü§î**"
   ],
   "id": "b4c6e7ab-f5da-4d34-b528-b56f3869b631"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Now that we are done with the **ResNet** model which we consider our baseline, we can start fine-tuning the the **ViT** model."
   ],
   "id": "b46caa20-3671-42fc-8166-7b7cf763957a"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
