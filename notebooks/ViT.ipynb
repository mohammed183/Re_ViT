{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the Vision Transformer\n",
    "\n",
    "In this notebook we use the pretrained ViT-L/16 model on the ImageNet-21k dataset which contains about 14 million images. The model will be finetuned on different datasets which are used for image classification tasks and then compared the performance to the baseline model.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "b3a15733-4612-4d5d-8e8e-73bbbfb98836"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the pretrained weights on hugging face which are the same weights provided by the authors but translated to be used in Pytorch. The next few cells show the functions used for finetuning the **ViT-L/16** model on different datasets.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "7bb12f1b-d70f-4a9b-90bf-a9e727d82b36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install hugging face transformer <<< Move later to requirements >>>\n",
    "!pip install transformers"
   ],
   "id": "748cc874-48cb-4e39-afb4-62066d24cc3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTModel\n",
    "from torchvision import transforms, datasets, models"
   ],
   "id": "121c887a-e90d-45f6-84a7-7e48e7537842"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for transformer\n",
    "def get_vit_loaders(dataset=\"imagenet\", batch_size=64):\n",
    "    \"\"\"\n",
    "    This loads the whole dataset into memory and returns train and test data to\n",
    "    be used by the Vision Transformer\n",
    "    @param dataset (string): dataset name to load\n",
    "    @param batch_size (int): batch size for training and testing\n",
    "\n",
    "    @returns dict() with train and test data loaders with keys `train_loader`, `test_loader`\n",
    "    \"\"\"\n",
    "    # Normalization using channel means\n",
    "    normalize_transform = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "    # Creating transform function\n",
    "    train_transform =transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), normalize_transform])\n",
    "\n",
    "    # Test transformation function\n",
    "    test_transform =transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), normalize_transform])\n",
    "\n",
    "    # Load the dataset from torchvision datasets \n",
    "    if dataset == \"imagenet\":\n",
    "        # Load ImageNet\n",
    "        original_train_dataset = datasets.ImageNet(root=os.path.join('data', 'imagenet_data'),\n",
    "                                             split='train', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.ImageNet(root=os.path.join('data', 'imagenet_data'),\n",
    "                                             split='val', transform=test_transform, download=True)\n",
    "    elif dataset == \"cifar10\":\n",
    "        # Load CIFAR-10 \n",
    "        original_train_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == \"cifar100\":\n",
    "        # Load CIFAR-100 \n",
    "        original_train_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == \"oxford_pets\":\n",
    "        # Load Oxford-IIIT Pets \n",
    "        original_train_dataset = datasets.OxfordPets(root=os.path.join('data', 'oxford_pets_data'),\n",
    "                                             image_set='train', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.OxfordPets(root=os.path.join('data', 'oxford_pets_data'),\n",
    "                                             image_set='test', transform=test_transform, download=True)\n",
    "    elif dataset == \"oxford_flowers\":\n",
    "        # Load Oxford Flowers-102\n",
    "        original_train_dataset = datasets.OxfordFlowers102(root=os.path.join('data', 'oxford_flowers_102_data'),\n",
    "                                             split='train', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.OxfordFlowers102(root=os.path.join('data', 'oxford_flowers_102_data'),\n",
    "                                             split='test', transform=test_transform, download=True)\n",
    "    else:\n",
    "        # Raise an error if the dataset is not valid\n",
    "        raise ValueError(\"Invalid dataset name. Please choose one of the following: imagenet, cifar10, \\\n",
    "         cifar100, oxford_pets, oxford_flowers\")\n",
    "\n",
    "    # Creating data loaders\n",
    "    loader_args = {\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_train_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_test_dataset,\n",
    "        shuffle=False,\n",
    "        **loader_args)\n",
    "\n",
    "    return {\"train_loader\": train_loader,\n",
    "            \"test_loader\": test_loader}"
   ],
   "id": "2cd0e325-b707-4805-a85e-44c38eaacb29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes predictions and true values to return accuracies\n",
    "def get_accuracy(logit, true_y):\n",
    "    pred_y = torch.argmax(logit, dim=1)\n",
    "    return (pred_y == true_y).float().mean()\n",
    "\n",
    "# This Function is used to evaluate the model\n",
    "def evaluate_on_test(model, test_loader, device):\n",
    "    # Evaluate the model on all the test batches\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for batch_idx, (data_x, data_y) in enumerate(test_loader):\n",
    "        data_x = data_x.to(device)\n",
    "        data_y = data_y.to(device)\n",
    "\n",
    "\n",
    "        model_y = model.classifier(model(data_x).pooler_output)\n",
    "        loss = criterion(model_y, data_y)\n",
    "        batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "        accuracies.append(batch_accuracy.item())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Store test accuracy for plotting\n",
    "    test_loss = np.mean(losses)\n",
    "    test_accuracy = np.mean(accuracies)\n",
    "    test_acc.append(test_accuracy*100)\n",
    "    return test_accuracy, test_loss"
   ],
   "id": "6a3d4e58-fce4-445e-a862-c1e4824628fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model and return train and test accuracies\n",
    "def train_vit_model(title=\"\", loaders, model_name='google/vit-base-patch16-224-in21k',\n",
    "                         lr=0.003, epochs=10, random_seed=42, save=False):\n",
    "\n",
    "    # Create experiment directory \n",
    "    experiment_dir = os.path.join('experiments/exp1', title)\n",
    "\n",
    "    # make experiment directory\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "    # Set the seed\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Get num_classes\n",
    "    num_classes = len(loaders[\"train_loader\"].dataset.classes)\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    model = ViTModel.from_pretrained(model_name)\n",
    "    # Create a new linear layer with num_classes\n",
    "    new_classifier = torch.nn.Linear(model.config.hidden_size, num_classes)\n",
    "    # Assign it to the model.classifier attribute\n",
    "    model.classifier = new_classifier\n",
    "    # Move the model to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Arrays to hold accuracies\n",
    "    test_acc = [0]\n",
    "    train_acc = [0]\n",
    "\n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "\n",
    "        # Calculate loss and gradients for models on every training batch\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"train_loader\"]):\n",
    "            data_x = data_x.to(device)\n",
    "            data_y = data_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model_y = model.classifier(model(data_x).pooler_output)\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "            # Perform back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Store training accuracy for plotting\n",
    "        train_loss = np.mean(losses)\n",
    "        train_accuracy = np.mean(accuracies)\n",
    "        train_acc.append(train_accuracy*100)\n",
    "\n",
    "        print(\"Train accuracy: {} Train loss: {}\".format(train_accuracy, train_loss))\n",
    "\n",
    "        # Evaluate the model on all the test batches\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"test_loader\"]):\n",
    "            data_x = data_x.to(device)\n",
    "            data_y = data_y.to(device)\n",
    "\n",
    "\n",
    "            model_y = model.classifier(model(data_x).pooler_output)\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Store test accuracy for plotting\n",
    "        test_loss = np.mean(losses)\n",
    "        test_accuracy = np.mean(accuracies)\n",
    "        test_acc.append(test_accuracy*100)\n",
    "        print(\"Test accuracy: {} Test loss: {}\".format(test_accuracy, test_loss))\n",
    "\n",
    "    # Save the final model\n",
    "    if save:\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, os.path.join(experiment_dir, f'Vit-L/16{title}.pt'))\n",
    "\n",
    "    # return the accuracies\n",
    "    return train_acc, test_acc"
   ],
   "id": "8dd05258-5942-4af5-b588-09a26b843488"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a dataloader as parameter and plots 2 rows each contains 5 images\n",
    "def plot_images_from_dataloader(dataloader):\n",
    "    # Get the first batch of images and labels from the dataloader\n",
    "    images, labels = next(iter(dataloader))\n",
    "    classes = dataloader.dataset.classes\n",
    "    # Create a figure with 2 rows and 5 columns\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    # Loop over the axes and plot each image\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Get the image and label at index i\n",
    "        image = images[i]\n",
    "        label = classes[labels[i]]\n",
    "        # Unnormalize the image\n",
    "        image = image / 2 + 0.5\n",
    "        # Convert the image to numpy array\n",
    "        image = image.numpy()\n",
    "        # Transpose the image to have the channel dimension last\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        # Plot the image on the axis\n",
    "        ax.imshow(image)\n",
    "        # Set the title of the axis to the label\n",
    "        ax.set_title(label)\n",
    "        # Turn off the axis ticks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    # Show the plot\n",
    "    plt.show()"
   ],
   "id": "c2f31761-30d5-42f3-827e-f97fa4f2050d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageNet\n",
    "\n",
    "The ImageNet dataset consists of **1000** object classes and contains **1,281,167** training images, **50,000** validation images and **100,000** test images. The images vary in resolution but it is common practice to train deep learning models on sub-sampled images of **256x256**pixels. This dataset is widely used for image classification and localization tasks and has been the benchmark for many state-of-the-art algorithms.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "4d9d3065-a215-48c9-9291-ecf61d053050"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the ImageNet dataset\n",
    "loader = get_vit_loaders(dataset=\"imagenet\", batch_size=32)\n",
    "plot_images_from_dataloader(loader[\"test_loader\"])"
   ],
   "id": "321122a1-3511-4233-90b5-d350c149a6e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"CUDA Recognized\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Get num_classes\n",
    "num_classes = len(loader[\"train_loader\"].dataset.classes)\n",
    "\n",
    "# Get the fine tuned model on the ImageNet dataset\n",
    "model = ViTModel.from_pretrained('google/vit-large-patch16-224')\n",
    "# Move the model to the device\n",
    "model = model.to(device)"
   ],
   "id": "6554dcdf-acea-426d-83c0-aeae44ba5355"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Performance of the Ready fine tuned model\n",
    "train_acc_imagenet, _ = evaluate_on_test(model, loader[\"train_loader\"], device)\n",
    "test_acc_imagenet, _ = evaluate_on_test(model, loader[\"test_loader\"], device)"
   ],
   "id": "837460b3-5dbc-4ddb-b787-41f6fde9d883"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to a dictionary\n",
    "runs[\"imagenet\"] = { 'training_accuracy' : train_acc_imagenet,\n",
    "                       'test_accuracy' : test_acc_imagenet,\n",
    "                     }"
   ],
   "id": "f8a4dbd2-56b0-4a89-9635-535accd0e2e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/exp1/vit.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ],
   "id": "0f45c60c-d4dc-4d93-98d8-87a045da8bc3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset consists of **60,000 32x32** color images in **10** different classes. The 10 classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. There are **6,000** images per class, with **5,000** for training and **1,000** for testing. It is a popular benchmark for image classification and deep learning research.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "525d2a36-6faf-4e5e-82b8-12499bb0d64f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the CIFAR-10 dataset\n",
    "loader = get_vit_loaders(dataset=\"cifar10\", batch_size=32)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "d341c90b-482c-41c5-a4bc-43d2496b4858"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune the model on imagenet\n",
    "train_acc_cifar10, test_acc_cifar10 = train_vit_model(loaders=loader)"
   ],
   "id": "8c01193a-0202-4241-a715-6157894b45cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to a dictionary\n",
    "runs[\"cifar10\"] = { 'training_accuracy' : train_acc_cifar10,\n",
    "                       'test_accuracy' : test_acc_cifar10,\n",
    "                     }"
   ],
   "id": "90cefd5a-f050-457d-84fa-aaccb9188e7a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/exp1/vit.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ],
   "id": "34ccaa78-54cf-4538-9406-d04023eb1ff3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-100\n",
    "\n",
    "The CIFAR-100 dataset consists of **60,000 32x32** color images in **100** different classes. The 100 classes are grouped into 20 superclasses, such as aquatic mammals, flowers, insects, vehicles, etc. There are **600** images per class, with **500** for training and **100** for testing. It is also a commonly benchmark for image classification and deep learning research.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "358f8c52-a924-406a-8114-9be60c4141bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the CIFAR-100 dataset\n",
    "loader = get_vit_loaders(dataset=\"cifar100\", batch_size=32)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "2e7d4b16-a9cf-445d-814d-91d09b6cf0fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune the model on imagenet\n",
    "train_acc_cifar100, test_acc_cifar100 = train_vit_model(loaders=loader)"
   ],
   "id": "21ccf5a5-1a66-4ee0-86af-47174c98e077"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to a dictionary\n",
    "runs[\"cifar100\"] = { 'training_accuracy' : train_acc_cifar100,\n",
    "                       'test_accuracy' : test_acc_cifar100,\n",
    "                     }"
   ],
   "id": "06d4ddc3-45cf-41ca-bef2-de17f3a1a177"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/exp1/vit.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ],
   "id": "4b7d6b1d-f8cd-4d47-b5d5-40136745c916"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oxford-IIIT Pets\n",
    "\n",
    "The Oxford-IIIT Pets is a **37** category pet dataset with roughly **200** images for each class created by the Visual Geometry Group at Oxford. The images have large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI (region of interest), and pixel level trimap segmentation. The dataset is useful for fine-grained image classification and segmentation tasks.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "722477c8-1117-4f89-a1d0-0632465bc165"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the Oxford-IIIT Pets dataset\n",
    "loader = get_vit_loaders(dataset=\"oxford_pets\", batch_size=32)\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "e839cc25-5ed7-4df7-bc48-d57eab73f6fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune the model on imagenet\n",
    "train_acc_oxford_pets, test_acc_oxford_pets = train_vit_model(loaders=loader)"
   ],
   "id": "54a0e07e-462b-450c-9d64-e63390727bc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to a dictionary\n",
    "runs[\"oxford_pets\"] = { 'training_accuracy' : train_acc_oxford_pets,\n",
    "                       'test_accuracy' : test_acc_oxford_pets,\n",
    "                     }"
   ],
   "id": "fb9f3389-11d3-463d-8366-f0d45173f6ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/exp1/vit.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ],
   "id": "51aa80fa-693e-4f91-b6f1-ce47643d5b42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oxford Flowers-102\n",
    "\n",
    "The Oxford Flowers-102 dataset consists of **102** flower categories commonly occurring in the United Kingdom. Each class consists of between **40 and 258** images. The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories. The dataset also provides image labels, segmentations, and distances based on shape and color features.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "701bc986-4619-4d98-a4cb-a068b9e4fdb3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the Oxford Flowers-102 Pets dataset\n",
    "loader = get_vit_loaders(dataset=\"oxford_flowers\", batch_size=32)\n",
    "\n",
    "# We initialize the flowers names as they are not on Pytorch (used for plotting)\n",
    "flower_classes = ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',\n",
    " 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle',\n",
    " 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower',\n",
    " 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary',\n",
    " 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke',\n",
    " 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly',\n",
    " 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy',\n",
    " 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy',\n",
    " 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura',\n",
    " 'geranium', 'orange dahlia', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan',\n",
    " 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy',\n",
    " 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily',\n",
    " 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia',\n",
    " 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea',\n",
    " 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']\n",
    "\n",
    "# Save the Class names in the dataset\n",
    "loader[\"train_loader\"].dataset.classes = flower_classes\n",
    "# Plot dataset\n",
    "plot_images_from_dataloader(loader[\"train_loader\"])"
   ],
   "id": "2c07da90-3156-4345-b085-163c377f48ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune the model on imagenet\n",
    "train_acc_oxford_flowers, test_acc_oxford_flowers = train_vit_model(loaders=loader)"
   ],
   "id": "cc8b11d6-e3b3-4c29-b907-579f7f0ef927"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to a dictionary\n",
    "runs[\"oxford_flowers\"] = { 'training_accuracy' : train_acc_oxford_flowers,\n",
    "                       'test_accuracy' : test_acc_oxford_flowers,\n",
    "                     }"
   ],
   "id": "cee5262e-39f1-4d4e-a8ae-8f5e9299570f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/exp1/vit.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ],
   "id": "09f92034-20f2-4898-b243-663731e0c08d"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
