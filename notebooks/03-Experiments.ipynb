{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "In this section, we will attempt to verify the qualitative and quantitative aspects of each claim. We will indicate which claims cannot be verified due to the lack of the material published by the authors. We will mainly use pretrained models published to verify these claims. Trying to reproduce such results without pretrained models can be very expensive as the computational requirement is huge.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "b3c84d80-175c-4666-a52a-c17fe4d4215c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the experiments, let‚Äôs briefly discuss some of the common challenges that we will encounter in each of them. We will elaborate on these challenges later in the corresponding notebooks, but for now, here are some of the main issues that you should be aware of:\n",
    "\n",
    "-   The models are **very large and memory-intensive**, requiring a GPU with at least 16GB of RAM for the ViT models and at least 24GB of RAM for the ResNet models.\n",
    "-   The authors used a **very large batch size and a huge number of steps**, which can be very costly to reproduce.\n",
    "-   The resolutions reported in the paper are **not consistent with the ones provided in the code**.\n",
    "-   The fine-tuning learning rate for each dataset is **not reported in the paper**, but only the values used for grid search are given.\n",
    "-   The learning rate scheduler used in the code is **different from the one described in the paper**.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "cabd70d0-ac97-4881-b798-f49c532af67a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting any of the experiments, we need to download the **ImageNet-1k** validation data to be able to verify the results on the **ImageNet-1k** dataset as it is *not available* in `torchvision.datasets`.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "2cfcf285-f08f-4b1d-8c35-6230586bccdf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ImageNet-1k validation dataset\n",
    "!gdown 1xAO6pGcJqvTtbwcdlVWlcNSDjLkHZtnA\n",
    "!unzip val.zip"
   ],
   "id": "8c185942-88e6-4ee2-ae3a-5014622621fa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "d6684943-e303-46e8-be2a-5269d7d37f06"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1:\n",
    "\n",
    "In this experiment we want to reproduce the claim: *‚ÄúVision Transformer outperforms state of the art CNNs on various classification tasks after pretraining on large datasets‚Äù* by using the only available pretrained model in the table in that claim and compare it to the other model that are pretrained on the **ImageNet-21k** unlike in the original paper where the other models were pretrained on the **JFT-300M** private dataset.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "e08c3a35-72aa-467b-be7c-e9bbd8a1e68c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Experiment is split into two notebooks:\n",
    "\n",
    "-   [ResNet notebook](ResNet.ipynb): This notebook allows us to evaluate the performance of different **ResNet** models on various image classification datasets. The `model name` can be changed to try different models. The models in this notebook used for this experiment are pretrained on the **ImageNet-21k** dataset and are ready for fine-tuning.\n",
    "\n",
    "-   [ViT notebook](ViT.ipynb): This notebook allows us to evaluate the performance of different **Vision Transformer (ViT)** models on various image classification datasets. The `model name` can be changed to try different models. The models in this notebook are pretrained on the **ImageNet-21k** dataset and are ready for fine-tuning.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "78a5a519-9346-4237-b3a6-5fafd242418b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running both notebooks, now we can reproduce the table using the results stored in `resnet.json` and `vit.json`"
   ],
   "id": "bac7eb45-86e9-4c64-bfc0-f82282ce2f60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from both json files and create a table with results\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read from json file\n",
    "with open(\"experiments/resnet.json\", \"r\") as f:\n",
    "    resnet = json.load(f)\n",
    "# Read from json file\n",
    "with open(\"experiments/vit.json\", \"r\") as f:\n",
    "    vit = json.load(f)\n",
    "\n",
    "overall={}\n",
    "\n",
    "# Merge resnet and vit dictionaries into one overall dictionary\n",
    "overall['ResNet152x4'] = resnet\n",
    "overall['ViT-L/16'] = vit\n",
    "\n",
    "# Create a dataframe with the result to be in a table form\n",
    "df = pd.DataFrame.from_dict(overall).T\n",
    "\n",
    "# Display the dataframe\n",
    "display(df.style.set_properties(**{'text-align': 'center', 'border': '2px solid black', 'padding': '10px',\\\n",
    "                                   'font-size': '15px'}))"
   ],
   "id": "c54db174-545d-4fff-a688-0306ec44739c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "cb3b225f-d296-4e19-8ee6-3e74527a6dc0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try: üß™\n",
    "\n",
    "We have experimented with some fine-tuning hyperparameters that yielded good results, but we can explore more and try to improve the performance of the models. For example, we can try:\n",
    "\n",
    "-   Using **different learning rates** to see how sensitive the models are to this hyperparameter. A learning rate that is too high or too low can affect the convergence and accuracy of the models.\n",
    "-   Changing the **number of epochs** to see how it affects the final results. Changing the number of epochs might lead to better results, but also increase the risk of overfitting or underfitting.\n",
    "-   Checking the **sensitivity of the model to the random seed** by changing it. The random seed can influence the initialization of the weights and the shuffling of the data. Different seeds might result in different outcomes for the same model and dataset.\n",
    "\n",
    "The [**ConvNeXt**](https://arxiv.org/abs/2201.03545) paper was proposed after the **Vision Transformer (ViT)** model, which uses self-attention to process images. **ConvNeXt** combines the advantages of *ConvNets and self-supervised learning techniques*, such as masked autoencoders, to achieve better results than ViT. You can try to find the code and verify that these **CNNs** are actually able to get better results than ViT. You can use the [**ConvNeXt Code**](https://github.com/facebookresearch/ConvNeXt) from the official github which contains this [notebook](https://colab.research.google.com/drive/1CBYTIZ4tBMsVL5cqu9N_-Q3TBprqsfEO?usp=sharing) where you can test the model.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "2c467a56-18e3-41a5-be1e-cc1e31819ea3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional\n",
    "\n",
    "At this point we acheived our goals that was introduced in the first section of this material. The following experiments will verify the rest of the claims introduced in the claims section. You can try to validate these claims by following the steps described. However, it will not be as easy as the previous part as you will need to solve some of the challenges that was explained in the previous experiment yourself this time (eg. finding the learning rate, deciding which models to use, etc)."
   ],
   "id": "84619ded-c7fb-42d4-b736-9276857d3b1a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "c09524b3-06ed-4ad8-818e-ba2c078ba11d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2:\n",
    "\n",
    "In this experiment we want to verify the claim that states that *‚ÄúThe performance of the Vision Transformer on the classification task after fine tuning improves with the increase of the pretraining dataset size‚Äù* by using the available pretrained model as before. For this claim the authors compare the performance after pretraining on three datasets: **ImageNet-1k**, **ImageNet-21k** and **JFT-300M**. However, we cannot reproduce the results related to the **JFT-300M** dataset as before, so we will only use the model pretrained on the other two datasets. We mainly compare the results on the ImageNet-1k dataset but we can also extend the experiment and fine tune the model on the other datasets but this can be computationly expensive.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "68cfa25b-e728-468a-b35b-4bb3d5b7e529"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models available for this experiment are:\n",
    "\n",
    "|    Model    | Pretrained ImageNet | pretrained ImageNet-21k |\n",
    "|:-----------:|:-------------------:|:-----------------------:|\n",
    "| ResNet50x1  |         Yes         |           Yes           |\n",
    "| ResNet101x1 |         Yes         |           Yes           |\n",
    "| ResNet152x2 |         Yes         |           Yes           |\n",
    "| ResNet152x4 |         Yes         |           Yes           |\n",
    "|  ViT-B/32   |      Yes (SAM)      |           Yes           |\n",
    "|  ViT-B/16   |      Yes (SAM)      |           Yes           |\n",
    "|  ViT-L/32   |      Yes (SAM)      |           Yes           |\n",
    "|  ViT-L/16   |      Yes (SAM)      |           Yes           |\n",
    "\n",
    "We already have the results for the **ResNet-152x4** and **ViT-L/16** pretrained on the **ImageNet-21k**.\n",
    "\n",
    "Notice that the vision transformers pretrained on the **ImageNet-1k** use a different optimizer than described in the paper which will prevent us from validating the quantitative results of this claim. Moreover, these models are only compatible with the **JAX** framework, not with **PyTorch**. Therefore, we need to use **JAX** to load and use these models. To test the qualitative claim, we can choose any two models per dataset and compare their performance.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "50d33a2b-4eb3-45b0-9d9b-a2af0481a9ae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Experiment we will us the following three notebooks:\n",
    "\n",
    "-   [ResNet notebook](ResNet.ipynb): Thi is the same notebook from the previous experiment: We can change the model name by modifying the `model_name` variable in the code. The available models are listed in this [link](https://console.cloud.google.com/storage/browser/bit_models;tab=objects?prefix=&forceOnObjectsSortingFiltering=false). The models with ‚ÄòM‚Äô in their names are pretrained on **ImageNet-21k**, while the ones with ‚ÄòS‚Äô are pretrained on **ImageNet-1k**. The models with both ‚ÄòM‚Äô and ‚ÄòILSVRC2012‚Äô are finetuned on **ImageNet-1k**, while the others require finetuning.\n",
    "\n",
    "-   [ViT notebook](ViT.ipynb): This is the same notebook from the previous experiment. We can change the model name by modifying the `model_name` variable in the code. The available models are listed in this [link](https://huggingface.co/models?sort=trending&search=google%2Fvit-). We need to use the (384x384) models for **ImageNet-1k** and the (224x224) models for the other datasets.\n",
    "\n",
    "-   [ViT-JAX notebook](): This notebook allows us to evaluate the performance of different Vision Transformer (ViT) models on various image classification datasets using the JAX framework instead of PyTorch. We can change the model name by modifying the `model_name` variable in the code. The available models are listed in this [link](https://console.cloud.google.com/storage/browser/vit_models/sam?authuser=0&pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false).\n",
    "\n",
    "We will use these notebooks to obtain the results for each model and dataset combination and store them in JSON files. **Note: We need to be careful when naming the JSON files to avoid overwriting previous results.**\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "d050a5d9-bb5f-4732-9ffc-5dc93db28357"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the previous part, run the following cell to reproduce the table and compare it to the one in the claims section."
   ],
   "id": "5e07fdd6-a6e8-429c-8532-d22eb720170f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to store name of files used to create table\n",
    "runs = {}\n",
    "overall = {}\n",
    "file_names = [] # Add the names if the new files here\n",
    "\n",
    "# Loop over files\n",
    "for name in file_names:\n",
    "    # Read from json file\n",
    "    with open(f\"experiments/{name}.json\", \"r\") as f:\n",
    "        runs = json.load(f)\n",
    "    # Merge dictionary\n",
    "    overall[name] = runs\n",
    "\n",
    "# Create a dataframe with the result to be in a table form\n",
    "df = pd.DataFrame.from_dict(overall).T\n",
    "\n",
    "# Display the dataframe\n",
    "display(df.style.set_properties(**{'text-align': 'center', 'border': '2px solid black', 'padding': '10px',\\\n",
    "                                   'font-size': '15px'}))"
   ],
   "id": "49ede477-bf35-474d-bca5-921a155bce61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ],
   "id": "8f4b9191-c68d-4d63-bf24-03b78cca2145"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3:\n",
    "\n",
    "In this experiment we want to verify the claim that states that ‚ÄúThe hybrid Vision Transformer can perform better than both baseline and Vision Transformer after fine tuning it to different classification task‚Äù by using the available pretrained model as before. The models used by the authors are not available but there other models available that we can use.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "f0754ce8-576c-4a94-ab13-6d80f10f0b5c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models available for this experiment are all pretrained on the **ImageNet-21k** datasets:\n",
    "\n",
    "-   R50x1+ViT-B/16\n",
    "-   R50x1+ViT-L/32\n",
    "-   R50x1+ViT-L/16\n",
    "\n",
    "We can use any of these model and compare it to the results from the previous experiments to validate the qualitative version of the claim. However, we cannot validata the quantitative results as the models pretrained on the **JFT-300M** dataset are not available.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "6950c9d0-830d-48d0-b7fe-ec491cc8f79d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebooks used for this experiment:\n",
    "\n",
    "-   [ViT-JAX notebook](): This is the same notebook fromt the previous experiment. We can change the model name by modifying the `model_name` variable in the code. The available models are listed in this [link](https://console.cloud.google.com/storage/browser/vit_models?authuser=0&pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false). The model pretrained on the **ImageNet-21k** only without fine-tuning are available in the `imagenet21k/` folder, while the fine-tuned models on the **ImageNet-1k** are available in the `imagenet21k+imagenet2012/`folder.\n",
    "\n",
    "**Note: We need to be careful when naming the JSON files to avoid overwriting previous results.**\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "63000622-d5cb-42ed-b351-add6839258db"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the previous part, run the following cell to reproduce the table and compare it to the one in the claims section."
   ],
   "id": "032f58ec-1439-4a2e-aba1-1f19ba121e26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to store name of files used to create table\n",
    "runs = {}\n",
    "overall = {}\n",
    "file_names = [] # Add the names of the new files here\n",
    "\n",
    "# Loop over files\n",
    "for name in file_names:\n",
    "    # Read from json file\n",
    "    with open(f\"experiments/{name}.json\", \"r\") as f:\n",
    "        runs = json.load(f)\n",
    "    # Merge dictionary\n",
    "    overall[name] = runs\n",
    "\n",
    "# Create a dataframe with the result to be in a table form\n",
    "df = pd.DataFrame.from_dict(overall).T\n",
    "\n",
    "# Display the dataframe\n",
    "display(df.style.set_properties(**{'text-align': 'center', 'border': '2px solid black', 'padding': '10px',\\\n",
    "                                   'font-size': '15px'}))"
   ],
   "id": "aaeda6a5-17ce-4d1b-b5cf-a48d9fbfd8f4"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
